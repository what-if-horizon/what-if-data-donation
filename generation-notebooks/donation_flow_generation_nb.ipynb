{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc01ee1",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 1. Python code to dynamically create the Instagram script \n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89791373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- Load schema ---\n",
    "schema_df = pd.read_csv(\"/mnt/c/Users/arodilla/OneDrive - Universitat de Barcelona/BSC/WHAT-IF/SCHEMA_DATA/Merged_structures_IG_2_new.csv\")\n",
    "schema_df.columns = schema_df.columns.str.strip()\n",
    "schema_df = schema_df.dropna(subset=[\"variable\", \"value\"])\n",
    "\n",
    "# --- Helper to build access path for get_in/get_list ---\n",
    "def build_access_path(row):\n",
    "    parts = []\n",
    "    for i in range(1, 6):\n",
    "        key = f\"col_path_{i}\"\n",
    "        list_key = f\"{key}_LIST\"\n",
    "        if pd.notna(row.get(key)):\n",
    "            value = str(row[key]).strip()\n",
    "            if pd.notna(row.get(list_key)) and str(row[list_key]).strip().upper() == \"LIST\":\n",
    "                parts.append(f\"[{value}]\")\n",
    "            else:\n",
    "                parts.append(value)\n",
    "    return parts\n",
    "\n",
    "# --- Generate extractor function per json_name ---\n",
    "def generate_df_function_by_json(json_name: str, group: pd.DataFrame) -> str:\n",
    "    json_name_no_ext = json_name.replace(\".json\", \"\")\n",
    "    func_name = re.sub(r\"\\W|^(?=\\d)\", \"_\", json_name_no_ext.lower()) + \"_df\"\n",
    "    row_path = group[\"row_path\"].iloc[0]\n",
    "\n",
    "    col_paths = {}\n",
    "    for _, row in group.iterrows():\n",
    "        access_path = build_access_path(row)\n",
    "        if not access_path:\n",
    "            continue\n",
    "        col_name = re.sub(r\"\\W|^(?=\\d)\", \"_\", access_path[-1].replace(\" \", \"_\"))\n",
    "        col_paths[col_name] = access_path\n",
    "\n",
    "    # --- Build function body ---\n",
    "    lines = []\n",
    "    lines.append(f\"def {func_name}(file_input: list[str]) -> pd.DataFrame:\")\n",
    "    lines.append(f'    data = read_json(file_input, [\"*/{json_name}\"])')\n",
    "    lines.append(\"    rows = get_list(data, [\\\"\" + row_path + \"\\\"])\")\n",
    "    lines.append(\"    if not rows:\")\n",
    "    lines.append(\"        return pd.DataFrame([])\")\n",
    "    lines.append(\"    out = []\")\n",
    "    lines.append(\"    for entry in rows:\")\n",
    "    lines.append(\"        row = {}\")\n",
    "\n",
    "    for col, access_path in col_paths.items():\n",
    "        path_str = \"[\" + \", \".join([f'\"{p}\"' if not p.startswith(\"[\") else p for p in access_path]) + \"]\"\n",
    "        lines.append(f\"        row[\\\"{col}\\\"] = get_in(entry, {path_str})\")\n",
    "\n",
    "    lines.append(\"        out.append(row)\")\n",
    "    lines.append(\"    df = pd.DataFrame(out)\")\n",
    "    lines.append('    if \"time\" in df.columns:')\n",
    "    lines.append('        df[\"date\"] = pd.to_datetime(df[\"time\"], unit=\"s\").dt.strftime(\"%Y-%m-%d %H:%M:%S\")')\n",
    "    lines.append('        df = df.sort_values(\"date\")')\n",
    "    lines.append(\"    return df\\n\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# --- Generate explicit donation flow function ---\n",
    "def generate_donation_flow_function_explicit(schema_df: pd.DataFrame) -> str:\n",
    "    lines = []\n",
    "    lines.append(\"def create_donation_flow(file_input: list[str]):\")\n",
    "    lines.append('    \"\"\"Creates donation flow for Instagram JSON data.\"\"\"')\n",
    "    lines.append(\"    tables = []\\n\")\n",
    "\n",
    "    grouped = schema_df.groupby(\"json_name\")\n",
    "\n",
    "    for json_name, group in grouped:\n",
    "        name_base = re.sub(r\"\\W|^(?=\\d)\", \"_\", json_name.replace(\".json\", \"\").lower())\n",
    "        func_name = name_base + \"_df\"\n",
    "        table_name = name_base\n",
    "        lines.append(f\"    # Extract table: {table_name}\")\n",
    "        lines.append(\"    try:\")\n",
    "        lines.append(f\"        {table_name}_table = donation_table(\")\n",
    "        lines.append(f'            name=\"{table_name}\",')\n",
    "        lines.append(f\"            df={func_name}(file_input),\")\n",
    "        lines.append(f'            title={{\\\"en\\\": \\\"{table_name}\\\", \\\"nl\\\": \\\"{table_name}\\\"}},')\n",
    "        lines.append(\"        )\")\n",
    "        lines.append(f\"        tables.append({table_name}_table)\")\n",
    "        lines.append(\"    except Exception as e:\")\n",
    "        lines.append(f\"        logger.warning(f\\\"Skipping {table_name}: {{e}}\\\")\\n\")\n",
    "\n",
    "    lines.append(\"    if tables:\")\n",
    "    lines.append(\"        return donation_flow(\")\n",
    "    lines.append('            id=\"instagram\",')\n",
    "    lines.append(\"            tables=tables\")\n",
    "    lines.append(\"        )\")\n",
    "    lines.append(\"    else:\")\n",
    "    lines.append(\"        return None\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# --- Generate all extractors ---\n",
    "generated_functions = [\n",
    "    generate_df_function_by_json(json_name, group)\n",
    "    for json_name, group in schema_df.groupby(\"json_name\")\n",
    "]\n",
    "\n",
    "# --- Save to output file ---\n",
    "output_file = \"/mnt/c/Users/arodilla/OneDrive - Universitat de Barcelona/BSC/WHAT-IF/SCHEMA_DATA/instagram_generated_extractors.py\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Auto-generated Instagram extractors\\n\\n\")\n",
    "    f.write(\"import pandas as pd\\n\")\n",
    "    f.write(\"import logging\\n\")\n",
    "    f.write(\"from port.helpers.donation_flow import donation_table, donation_flow\\n\")\n",
    "    f.write(\"from port.helpers.readers import read_json\\n\")\n",
    "    f.write(\"from port.helpers.parsers import get_in, get_list\\n\\n\")\n",
    "    f.write(\"logger = logging.getLogger(__name__)\\n\\n\")\n",
    "    f.write(\"\\n\\n\".join(generated_functions))\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(generate_donation_flow_function_explicit(schema_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575df40d",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 2. Python code to dynamically create the Facebook script \n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac90e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import inspect\n",
    "import keyword \n",
    "\n",
    "# Load schema       - Alt Path (/mnt/c/Users/arodilla/Downloads/Merged_structures_IG.csv)\n",
    "schema_df = pd.read_csv('/mnt/c/Users/arodilla/OneDrive - Universitat de Barcelona/BSC/WHAT-IF/SCHEMA_DATA/Merged_structures_FB.csv')\n",
    "schema_df.columns = schema_df.columns.str.strip()\n",
    "schema_df = schema_df.dropna(subset=[\"variable\", \"value\"])\n",
    "\n",
    "# Helper function to flatten the schema and map to simplified column names\n",
    "def flatten_schema(schema: dict, parent_key: str = '') -> dict:\n",
    "    \"\"\"Recursively flattens nested schema dicts to dot notation paths and returns user-friendly column names.\"\"\"\n",
    "    items = {}\n",
    "    for k, v in schema.items():\n",
    "        new_key = f\"{parent_key}.{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.update(flatten_schema(v, new_key))\n",
    "        else:\n",
    "            items[new_key] = v\n",
    "    return items\n",
    "\n",
    "def generate_df_function_by_json(json_name: str, group: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generates a parsing function per json_name, using 'row_path' for row selection\n",
    "    and up to 5 levels of nested 'col_path_*' fields for column mapping.\n",
    "    \"\"\"\n",
    "    json_name_no_ext = json_name.replace(\".json\", \"\")\n",
    "    row_path = group[\"row_path\"].iloc[0]  # updated to use new column\n",
    "\n",
    "    # Clean function name\n",
    "    func_name = re.sub(r\"\\W|^(?=\\d)\", \"_\", json_name_no_ext.lower()) + \"_df\"\n",
    "\n",
    "    col_paths = {}\n",
    "    col_path_fields = [\"col_path_1\", \"col_path_2\", \"col_path_3\", \"col_path_4\", \"col_path_5\"]\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        # Extract all non-null parts of the path\n",
    "        path_parts = [str(row[col]) for col in col_path_fields if pd.notna(row.get(col))]\n",
    "        if not path_parts:\n",
    "            continue\n",
    "\n",
    "        # The column name is the last part\n",
    "        col_name = path_parts[-1]\n",
    "        col_name = re.sub(r\"\\W|^(?=\\d)\", \"_\", col_name.strip().replace(\" \", \"_\"))\n",
    "\n",
    "        # If it's a Python keyword, add underscore\n",
    "        if keyword.iskeyword(col_name):\n",
    "            col_name += \"_\"\n",
    "\n",
    "        json_path = \".\".join(path_parts)\n",
    "        col_paths[col_name] = [json_path]\n",
    "\n",
    "    # Start function definition\n",
    "    lines = []\n",
    "    lines.append(f\"def {func_name}(file_input: list[str]) -> pd.DataFrame:\")\n",
    "    lines.append(f'    data = read_json(file_input, [\"*/{json_name_no_ext}.json\"])')\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"    df = parse_json(data,\")\n",
    "    lines.append(f'        row_path=[\"$.{row_path}\"],')\n",
    "    lines.append(\"        col_paths=dict(\")\n",
    "\n",
    "    for col_name, path in col_paths.items():\n",
    "        lines.append(f'        {col_name} = {path},')\n",
    "\n",
    "    lines.append(\"        )\")\n",
    "    lines.append(\"    )\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    lines.append('    if \"time\" in df.columns:')\n",
    "    lines.append('        df[\"date\"] = pd.to_datetime(df[\"time\"], unit=\"s\").dt.strftime(\"%Y-%m-%d %H:%M:%S\")')\n",
    "    lines.append('        df = df.sort_values(\"date\")')\n",
    "    lines.append(\"\")\n",
    "\n",
    "    lines.append(\"    return df\\n\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def generate_donation_flow_function_explicit(schema_df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generates an explicit version of create_donation_flow that includes hardcoded try/except\n",
    "    blocks per known extractor function.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    lines.append(\"def create_donation_flow(file_input: list[str]):\")\n",
    "    lines.append('    \"\"\"')\n",
    "    lines.append(\"    Creates a donation flow for Facebook data, explicitly trying each extractor function.\")\n",
    "    lines.append(\"    Only creates tables for data that's available in the provided files.\")\n",
    "    lines.append('    \"\"\"')\n",
    "    lines.append(\"    tables = []\")\n",
    "    lines.append(\"    #print(file_input)\\n\")\n",
    "\n",
    "    grouped = schema_df.groupby(\"json_name\")\n",
    "\n",
    "    for json_name, group in grouped:\n",
    "        #name_base = json_name.replace(\".json\", \"\").replace(\"'\",\"_\").replace(\"-\",\"_\").lower()\n",
    "        name_base = re.sub(r\"\\W|^(?=\\d)\", \"_\", json_name.replace(\".json\", \"\").lower())\n",
    "        func_name = re.sub(r\"\\W|^(?=\\d)\", \"_\", name_base) + \"_df\"\n",
    "        table_name = name_base\n",
    "\n",
    "        # Basic fallback titles (could be improved by a proper map)\n",
    "        #raw_title = group[\"value\"].iloc[0] if pd.notna(group[\"value\"].iloc[0]) else table_name.replace(\"_\", \" \").title()\n",
    "        #escaped_title = raw_title.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n",
    "        escaped_title = table_name\n",
    "        english_title = escaped_title\n",
    "        dutch_title = escaped_title  # or translate if needed\n",
    "\n",
    "        lines.append(f\"    # {english_title}\")\n",
    "        lines.append(\"    try:\")\n",
    "        lines.append(f\"        {table_name}_table = donation_table(\")\n",
    "        lines.append(f'            name=\"{table_name}\",')\n",
    "        lines.append(f\"            df={func_name}(file_input),\")\n",
    "        lines.append(f'            title={{\\\"en\\\": \\\"{english_title}\\\", \\\"nl\\\": \\\"{dutch_title}\\\"}},')\n",
    "        lines.append(\"        )\")\n",
    "        lines.append(f\"        tables.append({table_name}_table)\")\n",
    "        lines.append(\"    except Exception as e:\")\n",
    "        lines.append(f'        #print(f\\\"Skipping {table_name}: {{e}}\\\")')\n",
    "        lines.append(\"        pass\\n\")\n",
    "\n",
    "    lines.append(\"    # Only create the donation flow if we have at least one table\")\n",
    "    lines.append(\"    if tables:\")\n",
    "    lines.append(\"        return donation_flow(\")\n",
    "    lines.append('            id=\\\"facebook\\\",')\n",
    "    lines.append(\"            tables=tables\")\n",
    "    lines.append(\"        )\")\n",
    "    lines.append(\"    else:\")\n",
    "    lines.append('        #print(\\\"No tables could be generated from the provided files\\\")')\n",
    "    lines.append(\"        return None\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Group by json_name (1 table per JSON file)\n",
    "generated_functions = [\n",
    "    generate_df_function_by_json(json_name, group)\n",
    "    for json_name, group in schema_df.groupby(\"json_name\")\n",
    "]\n",
    "\n",
    "# Function to create donation flow (remains the same)\n",
    "def create_donation_flow(file_input: list[str]):\n",
    "    \"\"\"\n",
    "    Creates a donation flow using pre-defined extractors applied to file_input ZIP.\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "\n",
    "    # List of all available extractor functions dynamically\n",
    "    extraction_functions = {}\n",
    "\n",
    "    for row in schema_df.iterrows():\n",
    "        filename = row[1][\"variable\"].split(\"/\")[-1]\n",
    "        func_name = re.sub(r\"\\W|^(?=\\d)\", \"_\", filename.replace(\".json\", \"\").lower()) + \"_df\"\n",
    "        extraction_functions[filename] = globals().get(func_name)\n",
    "\n",
    "    # Run the extractors\n",
    "    for filename, extractor_func in extraction_functions.items():\n",
    "        try:\n",
    "            df = pd.DataFrame(extractor_func(file_input))\n",
    "            if not df.empty:\n",
    "                table_name = filename.replace(\".json\", \"\").capitalize()\n",
    "                tables.append(\n",
    "                    donation_table(\n",
    "                        name=table_name,\n",
    "                        df=df,\n",
    "                        title={\"en\": table_name}\n",
    "                    )\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error running {extractor_func.__name__}: {e}\")\n",
    "\n",
    "    return donation_flow(\n",
    "        id=\"Facebook\",\n",
    "        tables=tables\n",
    "    )\n",
    "\n",
    "# Save the generated functions to a file\n",
    "all_code = \"\\n\\n\".join(generated_functions)\n",
    "# Get the source code of the create_donation_flow function as a string\n",
    "donation_flow_function_str = generate_donation_flow_function_explicit(schema_df)\n",
    "\n",
    "# Save the generated functions to a file\n",
    "all_code = \"\\n\\n\".join(generated_functions)\n",
    "with open(\"src/framework/processing/py/port/helpers/facebook_generated_extractors.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Auto-generated Facebook extractors\\n\\n\")\n",
    "    f.write(\"import pandas as pd\\n\")\n",
    "    f.write(\"import logging\\n\")\n",
    "    f.write(\"from port.helpers.donation_flow import donation_table, donation_flow\\n\")\n",
    "    f.write(\"from port.helpers.readers import read_json\\n\")\n",
    "    f.write(\"from port.helpers.parsers import parse_json\\n\\n\")\n",
    "    f.write(\"logger = logging.getLogger(__name__)\\n\\n\")\n",
    "    f.write(all_code)\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(donation_flow_function_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136eb70e",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 3. Python code to dynamically create the TikTok script \n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf86f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractors written to /home/larodilla/BSC/WHATIF/what-if-data-donation/src/framework/processing/py/port/helpers/tiktok_generated_extractors.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import inspect\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------\n",
    "# Utility Functions\n",
    "# -----------------------\n",
    "def get_in(d: dict, *keys):\n",
    "    for key in keys:\n",
    "        if isinstance(d, dict):\n",
    "            d = d.get(key)\n",
    "        else:\n",
    "            return None\n",
    "    return d\n",
    "\n",
    "def get_list(d: dict, *keys):\n",
    "    val = get_in(d, *keys)\n",
    "    return val if isinstance(val, list) else []\n",
    "\n",
    "def snake_case(name: str) -> str:\n",
    "    return name.lower().replace(\"-\", \"_\").replace(\".json\", \"\").replace(\".js\", \"\").replace(\" \", \"_\")\n",
    "\n",
    "# -----------------------\n",
    "# Load Schema\n",
    "# -----------------------\n",
    "schema_path = '/mnt/c/Users/arodilla/OneDrive - Universitat de Barcelona/BSC/WHAT-IF/SCHEMA_DATA/Merged_structures_TT_new_2.csv'\n",
    "schema_df = pd.read_csv(schema_path)\n",
    "schema_df.columns = schema_df.columns.str.strip()\n",
    "\n",
    "# -----------------------\n",
    "# Extract Field Paths\n",
    "# -----------------------\n",
    "def build_field_mappings(schema_group: pd.DataFrame):\n",
    "    static_fields = []\n",
    "    list_blocks = {}\n",
    "\n",
    "    for _, row in schema_group.iterrows():\n",
    "        path = []\n",
    "        list_path = []\n",
    "        last_key = None\n",
    "        row_path = row['row_path']\n",
    "        path.append(row_path)\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            col_val = row.get(f'col_path_{i}')\n",
    "            list_status = row.get(f'col_path_{i}_LIST', 'NO LIST')\n",
    "            if pd.notna(col_val):\n",
    "                path.append(col_val)\n",
    "                if list_status == 'LIST':\n",
    "                    list_path = path.copy()\n",
    "\n",
    "        last_key = path[-1] if path else None\n",
    "\n",
    "        if list_path:\n",
    "            list_path_tuple = tuple(list_path[:-1])\n",
    "            field = list_path[-1]\n",
    "            list_blocks.setdefault(list_path_tuple, set()).add(field)\n",
    "        elif last_key:\n",
    "            static_fields.append(path)\n",
    "\n",
    "    return static_fields, list_blocks\n",
    "\n",
    "# -----------------------\n",
    "# Generate Extractor Function\n",
    "# -----------------------\n",
    "def generate_df_function(file_folder_name: str, group: pd.DataFrame) -> str:\n",
    "    original_root_key = file_folder_name\n",
    "    func_name = f\"{snake_case(file_folder_name)}_df\"\n",
    "\n",
    "    lines = [\n",
    "        f\"def {func_name}(file_input: List[str]) -> pd.DataFrame:\",\n",
    "        \"    try:\",\n",
    "        \"        with open(file_input[0], 'r', encoding='utf-8') as f:\",\n",
    "        \"            data = json.load(f)\",\n",
    "        f\"        root_data = get_in(data, 'Activity','{original_root_key}')\",\n",
    "        f\"        if not root_data:\",\n",
    "        f\"            print(f'\\u26a0\\ufe0f No data found at path: {original_root_key}')\",\n",
    "        \"            return pd.DataFrame()\",\n",
    "        \"\",\n",
    "        \"        base_row = {}\",\n",
    "    ]\n",
    "\n",
    "    static_fields, list_blocks = build_field_mappings(group)\n",
    "\n",
    "    # Static fields\n",
    "    for path in static_fields:\n",
    "        field = path[-1]\n",
    "        path_str = \", \".join([f\"'{p}'\" for p in path[1:]])\n",
    "        lines.append(f\"        base_row['{field}'] = get_in(root_data, {path_str})\")\n",
    "\n",
    "    if not list_blocks:\n",
    "        lines.append(\"        return pd.DataFrame([base_row])\")\n",
    "    else:\n",
    "        lines.append(\"        all_records = []\")\n",
    "\n",
    "        for list_path, fields in list_blocks.items():\n",
    "            path_str = \", \".join([f\"'{p}'\" for p in list_path[1:]])\n",
    "            lines.append(f\"        items = get_list(root_data, {path_str})\")\n",
    "            lines.append(\"        for item in items:\")\n",
    "            lines.append(\"            row = base_row.copy()\")\n",
    "            lines.append(f\"            row['__source_list__'] = '{list_path[-1]}'\")\n",
    "            for field in sorted(fields):\n",
    "                lines.append(f\"            row['{field}'] = item.get('{field}', '.*?')\")\n",
    "            lines.append(\"            all_records.append(row)\")\n",
    "\n",
    "        lines.append(\"        return pd.DataFrame(all_records)\")\n",
    "\n",
    "    lines.extend([\n",
    "        \"    except Exception as e:\",\n",
    "        f\"        print(f'\\u274c Error in {func_name}:', e)\",\n",
    "        \"        return pd.DataFrame()\",\n",
    "        \"\"\n",
    "    ])\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# -----------------------\n",
    "# Generate Donation Flow\n",
    "# -----------------------\n",
    "def generate_donation_flow(schema_df: pd.DataFrame) -> str:\n",
    "    lines = [\n",
    "        \"def create_donation_flow(file_input: List[str]):\",\n",
    "        '    \"\"\"Create donation flow from TikTok JSON.\"\"\"',\n",
    "        \"    tables = []\",\n",
    "        \"\"\n",
    "    ]\n",
    "    for file_name, group in schema_df.groupby(\"row_path\"):\n",
    "        root_key = snake_case(file_name)\n",
    "        func_name = f\"{root_key}_df\"\n",
    "        lines.extend([\n",
    "            \"    try:\",\n",
    "            f\"        df = {func_name}(file_input)\",\n",
    "            \"        if not df.empty:\",\n",
    "            \"            tables.append(\",\n",
    "            f\"                donation_table(name='{root_key}', df=df, title={{'en': '{root_key}'}})\",\n",
    "            \"            )\",\n",
    "            \"    except Exception as e:\",\n",
    "            f\"        print(f'Error in {func_name}:', e)\",\n",
    "            \"\"\n",
    "        ])\n",
    "    lines.extend([\n",
    "        \"    if tables:\",\n",
    "        \"        return donation_flow(id='tiktok', tables=tables)\",\n",
    "        \"    else:\",\n",
    "        \"        return None\"\n",
    "    ])\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# -----------------------\n",
    "# Generate & Write Output\n",
    "# -----------------------\n",
    "generated_functions = [\n",
    "    generate_df_function(file_name, group)\n",
    "    for file_name, group in schema_df.groupby(\"row_path\")\n",
    "]\n",
    "\n",
    "donation_flow_code = generate_donation_flow(schema_df)\n",
    "\n",
    "output_path = Path(\"/home/larodilla/BSC/WHATIF/what-if-data-donation/src/framework/processing/py/port/helpers/tiktok_generated_extractors.py\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Auto-generated TikTok extractors\\n\\n\")\n",
    "    f.write(\"import pandas as pd\\n\")\n",
    "    f.write(\"import json\\n\")\n",
    "    f.write(\"import logging\\n\")\n",
    "    f.write(\"from port.helpers.donation_flow import donation_table, donation_flow\\n\")\n",
    "    f.write(\"from typing import List\\n\\n\")\n",
    "    f.write(inspect.getsource(get_in))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(inspect.getsource(get_list))\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(\"\\n\\n\".join(generated_functions))\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(donation_flow_code)\n",
    "\n",
    "print(f\"Extractors written to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac52dfef",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 4. Python code to dynamically create the Twitter script \n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c678a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractors written to /home/larodilla/BSC/WHATIF/what-if-data-donation/src/framework/processing/py/port/helpers/twitter_generated_extractors.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
      "/tmp/ipykernel_23423/3600514888.py:182: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import zipfile\n",
    "import logging\n",
    "import inspect\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# -----------------------\n",
    "# Utility Functions\n",
    "# -----------------------\n",
    "def get_in(d: dict, *keys):\n",
    "    for key in keys:\n",
    "        if isinstance(d, dict):\n",
    "            d = d.get(key)\n",
    "        else:\n",
    "            return None\n",
    "    return d\n",
    "\n",
    "def get_list(d: dict, *keys):\n",
    "    val = get_in(d, *keys)\n",
    "    return val if isinstance(val, list) else []\n",
    "\n",
    "def get_dict(d: dict, *keys):\n",
    "    val = get_in(d, *keys)\n",
    "    return val if isinstance(val, dict) else {}\n",
    "\n",
    "def snake_case(name: str) -> str:\n",
    "    return name.lower().replace(\"-\", \"_\").replace(\".json\", \"\").replace(\".js\", \"\").replace(\" \", \"_\")\n",
    "\n",
    "def extract_path(row):\n",
    "    path = []\n",
    "    for col in [\"row_path\"] + [f\"col_path_{i}\" for i in range(1, 6)]:\n",
    "        val = row.get(col)\n",
    "        if pd.notna(val) and str(val).strip().upper() != \"MISSING\":\n",
    "            path.append(str(val).strip())\n",
    "    return path\n",
    "\n",
    "def get_field_name(row):\n",
    "    for i in reversed(range(1, 6)):\n",
    "        val = row.get(f\"col_path_{i}\")\n",
    "        if pd.notna(val) and val != 'MISSING':\n",
    "            return str(val).strip()\n",
    "    return None\n",
    "\n",
    "def is_list_index(val):\n",
    "    return str(val).isdigit()\n",
    "\n",
    "# -----------------------\n",
    "# Load Schema\n",
    "# -----------------------\n",
    "schema_df = pd.read_csv(\"/mnt/c/Users/arodilla/OneDrive - Universitat de Barcelona/BSC/WHAT-IF/SCHEMA_DATA/Merged_structures_X_new_2.csv\")\n",
    "schema_df.columns = schema_df.columns.str.strip()\n",
    "schema_df = schema_df.dropna(subset=[\"json_name\", \"row_path\"])\n",
    "schema_df = schema_df[schema_df['row_path'] != 'No data']\n",
    "\n",
    "# -----------------------\n",
    "# Read JS File from ZIP\n",
    "# -----------------------\n",
    "def read_js(file_input: list[str], target_files: list[str]) -> list[dict]:\n",
    "    extracted_data = []\n",
    "    for zip_path in file_input:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            for target_file in target_files:\n",
    "                js_files = [f for f in z.namelist() if target_file in f]\n",
    "                if js_files:\n",
    "                    with z.open(js_files[0]) as raw_file:\n",
    "                        with io.TextIOWrapper(raw_file, encoding=\"utf8\") as text_file:\n",
    "                            lines = text_file.readlines()\n",
    "                        lines[0] = re.sub(r\"^.*? = \", \"\", lines[0])\n",
    "                        try:\n",
    "                            data = json.loads(\"\".join(lines))\n",
    "\n",
    "                            if isinstance(data,list):\n",
    "                                extracted_data.extend(data)\n",
    "                            else:\n",
    "                                extracted_data.append(data)\n",
    "\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            logger.error(f\"Error decoding {target_file} in {zip_path}: {e}\")\n",
    "    return extracted_data\n",
    "\n",
    "# -----------------------\n",
    "# Generate Extractor Function\n",
    "# -----------------------\n",
    "def generate_df_function(json_name: str, group: pd.DataFrame) -> str:\n",
    "    root_key = json_name\n",
    "    func_name = f\"{snake_case(json_name)}_df\"\n",
    "\n",
    "    lines = [\n",
    "        f\"def {func_name}(file_input: list[str]) -> pd.DataFrame:\",\n",
    "        f\"    data = read_js(file_input, ['/{json_name}'])\",\n",
    "        \"    records = []\",\n",
    "        \"    for item in data:\",\n",
    "        \"        base_row = {}\",\n",
    "    ]\n",
    "\n",
    "    static_fields = []\n",
    "    list_blocks = {}\n",
    "    seen_static = set()\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        path = extract_path(row)\n",
    "\n",
    "        for i, key in enumerate(path):\n",
    "            if str(row.get(f\"col_path_{i+1}_LIST\", \"\")).strip().upper() == \"LIST\":\n",
    "                list_path = tuple(path[:i+1])\n",
    "                subfield_path = path[i+1:]\n",
    "                if subfield_path:\n",
    "                    list_blocks.setdefault(list_path, set()).add(tuple(subfield_path))\n",
    "                break\n",
    "        else:\n",
    "            field = path[-1]\n",
    "            if field not in seen_static:\n",
    "                static_fields.append(path)\n",
    "                seen_static.add(field)\n",
    "\n",
    "    for path in static_fields:\n",
    "        field = path[-1]\n",
    "        path_str = \"', '\".join(path)\n",
    "        lines.append(f\"        base_row['{field}'] = get_in(item, '{path_str}')\")\n",
    "\n",
    "    if not list_blocks:\n",
    "        lines.append(\"        records.append(base_row)\")\n",
    "    else:\n",
    "        for list_path, fields in list_blocks.items():\n",
    "            path_str = \"', '\".join(list_path)\n",
    "            lines += [\n",
    "                f\"        for entry in get_list(item, '{path_str}'):\",\n",
    "                \"            row = base_row.copy()\",\n",
    "                f\"            row['__source_list__'] = '{list_path[-1]}'\"\n",
    "            ]\n",
    "            for field_path in sorted(fields):\n",
    "                field_name = field_path[-1]\n",
    "                path_str = \"', '\".join(field_path)\n",
    "                lines.append(f\"            row['{field_name}'] = get_in(entry, '{path_str}')\")\n",
    "            lines.append(\"            records.append(row)\")\n",
    "\n",
    "    lines.append(\"    return pd.DataFrame(records)\\n\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# -----------------------\n",
    "# Generate Donation Flow\n",
    "# -----------------------\n",
    "def generate_donation_flow(schema_df: pd.DataFrame) -> str:\n",
    "    lines = [\n",
    "        \"def create_donation_flow(file_input: list[str]):\",\n",
    "        '    \"\"\"Create a donation flow for Twitter data.\"\"\"',\n",
    "        \"    tables = []\\n\"\n",
    "    ]\n",
    "    for json_name, group in schema_df.groupby(\"json_name\"):\n",
    "        func_name = f\"{snake_case(json_name)}_df\"\n",
    "        lines += [\n",
    "            f\"    try:\",\n",
    "            f\"        df = {func_name}(file_input)\",\n",
    "            f\"        if not df.empty:\",\n",
    "            f\"            tables.append(\",\n",
    "            f\"                donation_table(name='{snake_case(json_name)}', df=df, title={{'en': '{snake_case(json_name)}'}})\",\n",
    "            f\"            )\",\n",
    "            f\"    except Exception as e:\",\n",
    "            f\"        logger.error(f'Skipping {func_name}: {{e}}')\\n\"\n",
    "        ]\n",
    "\n",
    "    lines += [\n",
    "        \"    if tables:\",\n",
    "        \"        return donation_flow(id='twitter', tables=tables)\",\n",
    "        \"    else:\",\n",
    "        \"        return None\"\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# -----------------------\n",
    "# Write Output File\n",
    "# -----------------------\n",
    "generated_functions = [\n",
    "    generate_df_function(json_name, group)\n",
    "    for json_name, group in schema_df.groupby(\"json_name\")\n",
    "    if not group[[f\"col_path_{i}\" for i in range(1, 6)]].applymap(lambda v: pd.isna(v) or str(v).strip().upper() == \"MISSING\").all(axis=1).all()\n",
    "]\n",
    "\n",
    "donation_flow_function_str = generate_donation_flow(schema_df)\n",
    "\n",
    "output_path = Path(\"/home/larodilla/BSC/WHATIF/what-if-data-donation/src/framework/processing/py/port/helpers/twitter_generated_extractors.py\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Auto-generated Twitter extractors\\n\\n\")\n",
    "    f.write(\"import pandas as pd\\n\")\n",
    "    f.write(\"import json\\n\")\n",
    "    f.write(\"import logging\\n\")\n",
    "    f.write(\"import io\\n\")\n",
    "    f.write(\"import zipfile\\n\")\n",
    "    f.write(\"import re\\n\")\n",
    "    f.write(\"from port.helpers.donation_flow import donation_table, donation_flow\\n\\n\")\n",
    "    f.write(\"logger = logging.getLogger(__name__)\\n\\n\")\n",
    "    for fn in [get_in, get_list, get_dict, snake_case, extract_path, get_field_name]:\n",
    "        f.write(inspect.getsource(fn).strip() + \"\\n\\n\")\n",
    "    f.write(inspect.getsource(read_js).strip() + \"\\n\\n\")\n",
    "    f.write(\"\\n\\n\".join(generated_functions))\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(donation_flow_function_str)\n",
    "\n",
    "print(f\"Extractors written to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041b8ad",
   "metadata": {},
   "source": [
    "# ==================================================================================\n",
    "# 4. Python code to dynamically create the Youtube script \n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f01f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor functions written to: /home/larodilla/BSC/WHATIF/what-if-data-donation/src/framework/processing/py/port/helpers/youtube_generated_extractors.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import inspect\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import fnmatch\n",
    "import io\n",
    "import os\n",
    "\n",
    "# -----------------------\n",
    "# Utility Functions\n",
    "# -----------------------\n",
    "def read_csv_from_file_input(file_input: list[str], csv_filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file from a zip inside file_input.\n",
    "\n",
    "    Args:\n",
    "        file_input (list[str]): List of file paths, including the zip file.\n",
    "        csv_filename (str): Name of the CSV file inside the zip.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded DataFrame.\n",
    "    \"\"\"\n",
    "    for path in file_input:\n",
    "        if path.endswith('.zip'):\n",
    "            with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "                # Find matching file (e.g., endswith 'channel.csv')\n",
    "                for name in zip_ref.namelist():\n",
    "                    if name.endswith(csv_filename):\n",
    "                        with zip_ref.open(name) as f:\n",
    "                            try:\n",
    "                                return pd.read_csv(f, encoding='utf-8')\n",
    "                            except UnicodeDecodeError:\n",
    "                                f.seek(0)\n",
    "                                return pd.read_csv(f, encoding='latin1')  # fallback\n",
    "    raise FileNotFoundError(f\"{csv_filename} not found in ZIP files: {file_input}\")\n",
    "\n",
    "def get_in(d: dict, *keys):\n",
    "    for key in keys:\n",
    "        if isinstance(d, dict):\n",
    "            d = d.get(key)\n",
    "        elif isinstance(d, list) and isinstance(key, int) and len(d) > key:\n",
    "            d = d[key]\n",
    "        else:\n",
    "            return None\n",
    "    return d\n",
    "\n",
    "def get_list(d: dict, *keys):\n",
    "    val = get_in(d, *keys)\n",
    "    return val if isinstance(val, list) else []\n",
    "\n",
    "def snake_case(name: str) -> str:\n",
    "    return name.lower().replace(\"-\", \"_\").replace(\".json\", \"\").replace(\".js\", \"\").replace(\" \", \"_\")\n",
    "\n",
    "# -----------------------\n",
    "# Read JSON from ZIP\n",
    "# -----------------------\n",
    "def read_json(file_input: List[str], pattern: List[str]) -> List[dict]:\n",
    "    zip_path = file_input[0]\n",
    "    matched_data = []\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
    "        for pat in pattern:\n",
    "            for name in zipf.namelist():\n",
    "                if fnmatch.fnmatch(name, pat):\n",
    "                    with zipf.open(name) as f:\n",
    "                        content = json.load(f)\n",
    "                        if isinstance(content, list):\n",
    "                            matched_data.extend(content)\n",
    "                        else:\n",
    "                            matched_data.append(content)\n",
    "    return matched_data\n",
    "\n",
    "# -----------------------\n",
    "# Path Utilities\n",
    "# -----------------------\n",
    "def extract_path(row):\n",
    "    path = []\n",
    "    for col in [\"row_path\"] + [f\"col_path_{i}\" for i in range(1, 6)]:\n",
    "        val = row.get(col)\n",
    "        if pd.notna(val):\n",
    "            path.append(str(val).strip())\n",
    "    return path\n",
    "\n",
    "# -----------------------\n",
    "# Generate Extractor Function\n",
    "# -----------------------\n",
    "def generate_df_function(file_name: str, group: pd.DataFrame) -> str:\n",
    "    func_name = f\"{snake_case(file_name)}_df\"\n",
    "    json_path = f\"*/{file_name}\"\n",
    "\n",
    "    lines = [\n",
    "        f\"def {func_name}(file_input: List[str]) -> pd.DataFrame:\",\n",
    "        f\"    data = read_json(file_input, [\\\"{json_path}\\\"])\\n\",\n",
    "        \"    records = []\",\n",
    "        \"    for entry in data:\",\n",
    "        \"        records.append({\"\n",
    "    ]\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        path = extract_path(row)\n",
    "        field_name = \"_\".join(path)\n",
    "\n",
    "        # Handle list with .join\n",
    "        if path[-1] == \"activityControls\" or path[-1] == \"products\":\n",
    "            path_str = \"', '\".join(path)\n",
    "            lines.append(f\"            \\\"{field_name}\\\": \\\", \\\".join(get_list(entry, '{path_str}')),\")\n",
    "        # Handle nested fields\n",
    "        elif any(p.isdigit() for p in path):\n",
    "            index_path = [int(p) if p.isdigit() else f\"'{p}'\" for p in path]\n",
    "            index_path_str = \", \".join(str(p) for p in index_path)\n",
    "            lines.append(f\"            \\\"{field_name}\\\": get_in(entry, {index_path_str}),\")\n",
    "        # Handle top-level fields\n",
    "        elif len(path) == 1:\n",
    "            lines.append(f\"            \\\"{field_name}\\\": entry.get('{path[0]}'),\")\n",
    "        else:\n",
    "            path_str = \"', '\".join(path)\n",
    "            lines.append(f\"            \\\"{field_name}\\\": get_in(entry, '{path_str}'),\")\n",
    "\n",
    "    lines += [\n",
    "        \"        })\",\n",
    "        \"    df = pd.DataFrame(records)\",\n",
    "        \"    return df\\n\"\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# -----------------------\n",
    "# Generate Donation Flow\n",
    "# -----------------------\n",
    "def generate_donation_flow(schema_df: pd.DataFrame, schema_csv: pd.DataFrame) -> str:\n",
    "    lines = [\n",
    "        \"def create_donation_flow(file_input: List[str]):\",\n",
    "        '    \"\"\"Create donation flow from YouTube ZIP.\"\"\"',\n",
    "        \"    tables = []\",\n",
    "        \"\"\n",
    "    ]\n",
    "\n",
    "    # Add JSON-based tables\n",
    "    for file_name, _ in schema_df.groupby(\"json_name\"):\n",
    "        func = f\"{snake_case(file_name)}_df\"\n",
    "        table = snake_case(file_name)\n",
    "        lines += [\n",
    "            f\"    try:\",\n",
    "            f\"        df = {func}(file_input)\",\n",
    "            f\"        if not df.empty:\",\n",
    "            f\"            tables.append(donation_table(name='{table}', df=df, title={{'en': '{table}'}}))\",\n",
    "            f\"    except Exception:\",\n",
    "            f\"        pass\",\n",
    "            \"\"\n",
    "        ]\n",
    "\n",
    "    # Add CSV-based tables\n",
    "    csv_groups = schema_csv.groupby(\"Key\")  # Key = CSV file name\n",
    "\n",
    "    for csv_name, group in csv_groups:\n",
    "        table_name = os.path.splitext(csv_name)[0]  # Strip .csv\n",
    "        expected_columns = list(group[\"Values\"].unique())\n",
    "        lines += [\n",
    "            f\"    try:\",\n",
    "            f\"        df = read_csv_from_file_input(file_input, '/{csv_name}')\",  # uses your helper\n",
    "            f\"        expected_columns = {expected_columns}\",\n",
    "            f\"        existing_columns = [col for col in expected_columns if col in df.columns]\",\n",
    "            f\"        df = df[existing_columns]\",\n",
    "            f\"        if not df.empty:\",\n",
    "            f\"            tables.append(donation_table(name='{table_name}', df=df, title={{'en': '{table_name}'}}))\",\n",
    "            f\"    except Exception:\",\n",
    "            f\"        pass\",\n",
    "            \"\"\n",
    "        ]\n",
    "\n",
    "    lines += [\n",
    "        \"    if tables:\",\n",
    "        \"        return donation_flow(id='youtube', tables=tables)\",\n",
    "        \"    else:\",\n",
    "        \"        return None\"\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# -----------------------\n",
    "# Write Output to File\n",
    "# -----------------------\n",
    "def main():\n",
    "    schema_path = \"/mnt/c/Users/arodilla/OneDrive - Universitat de Barcelona/BSC/WHAT-IF/SCHEMA_DATA/Merged_structures_YT.csv\"  #  Update this path\n",
    "    schema_csv_path = \"/mnt/c/Users/arodilla/OneDrive - Universitat de Barcelona/BSC/WHAT-IF/SCHEMA_DATA/Merged_Column_Names_YT.csv\"\n",
    "    output_path = Path(\"/home/larodilla/BSC/WHATIF/what-if-data-donation/src/framework/processing/py/port/helpers/youtube_generated_extractors.py\")\n",
    "\n",
    "    schema_df = pd.read_csv(schema_path)\n",
    "    schema_df.columns = schema_df.columns.str.strip()\n",
    "\n",
    "    schema_csv = pd.read_csv(schema_csv_path)\n",
    "    schema_csv.columns = schema_csv.columns.str.strip()\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# Auto-generated YouTube extractors\\n\\n\")\n",
    "        f.write(\"import pandas as pd\\n\")\n",
    "        f.write(\"import json\\n\")\n",
    "        f.write(\"from typing import List\\n\")\n",
    "        f.write(\"from port.helpers.donation_flow import donation_table, donation_flow\\n\")\n",
    "        f.write(\"import zipfile\\n\")\n",
    "        f.write(\"import fnmatch\\n\\n\")\n",
    "        f.write(inspect.getsource(read_csv_from_file_input) + \"\\n\")\n",
    "        f.write(inspect.getsource(get_in) + \"\\n\")\n",
    "        f.write(inspect.getsource(get_list) + \"\\n\")\n",
    "        f.write(inspect.getsource(snake_case) + \"\\n\")\n",
    "        f.write(inspect.getsource(read_json) + \"\\n\")\n",
    "\n",
    "        for file_name, group in schema_df.groupby(\"json_name\"):\n",
    "            f.write(\"\\n\\n\" + generate_df_function(file_name, group))\n",
    "\n",
    "        f.write(\"\\n\\n\" + generate_donation_flow(schema_df, schema_csv))\n",
    "\n",
    "    print(f\"Extractor functions written to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
