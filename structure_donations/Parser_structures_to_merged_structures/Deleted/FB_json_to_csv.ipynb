{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Facebook Data Structures into a Schema_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The purpose of his jupyter notebook is to parse the collected data structures in earlier iterations of utilising the data donation tool into a schema_df which can be used to inform future iterations of the data donation tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries and the Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process_col_path()\n",
    "The 'process_col_path()' function checks whether the value in a row of for one of the column paths is actually a datatype and stores this value in a column data_type and replaces the original value with NA. The data types are the lowest level values in the JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types\n",
    "data_types = ['string', 'array', 'number', 'boolean', 'object', 'str', 'int', 'float', 'bool', 'dict', 'list']\n",
    "\n",
    "#Define the column names\n",
    "columns = ['col_path_1', 'col_path_2', 'col_path_3', 'col_path_4', 'col_path_5', 'col_path_6', 'col_path_7', 'col_path_8']\n",
    "\n",
    "\n",
    "\n",
    "# Define the function 'process_col_path()'\n",
    "def process_col_path(row, columns, data_types):\n",
    "\n",
    "    \"\"\"\n",
    "    row: Rows in the dataframe\n",
    "    columns: List of column names of column path columns \n",
    "    data_types: List of the values that are data types\n",
    "    \"\"\"\n",
    "\n",
    "    row['data_type'] = ''\n",
    "    for column in columns:\n",
    "\n",
    "        #If the value stored in the column is found in the list 'data_types', \n",
    "        if row[column] in data_types:\n",
    "            # this value is placed in the column 'data_type'\n",
    "            row['data_type'] = row[column]\n",
    "            # and the original value is replaced with NA\n",
    "            row[column] = np.nan\n",
    "\n",
    "        #If the value is not found in the 'data_types' list, the original value is returned\n",
    "        else:\n",
    "            row[column]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file_paths()\n",
    "The 'file_paths()' function splits up the paths to where the JSON file is stored in the folder and provides the name of the json file. \n",
    "- 'path_{1,2,3,4}': Column including the names of the {first, second, third, fourth} level folder where the JSON file is stored\n",
    "- 'json_name': Column including the name of the JSON file\n",
    "\n",
    "If the JSON name appears in the 'path_{1,2,3,4}' column, this name is replaced with NA and stored in the column 'json_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_paths(df):\n",
    "\n",
    "    #This line is included as in some of the structures generated through the data structure tool include the name of the zip which is a mistake\n",
    "    df['variable'] = df['variable'].str.replace('^facebook-[^/]+/', '', regex=True)\n",
    "    \n",
    "    # Create different columns for each part of the document path\n",
    "    df['path_1'] = df['variable'].str.split('/', n=1).str[0]\n",
    "    df['path_2'] = df['variable'].str.split('/', n=3).str[1]\n",
    "    df['path_3'] = df['variable'].str.split('/', n=3).str[2]\n",
    "    df['path_4'] = df['variable'].str.split('/', n=3).str[3]\n",
    "\n",
    "    # Create a column with the JSON name\n",
    "    df['json_name'] = df['variable'].str.rsplit('/', n=1).str[-1]\n",
    "\n",
    "\n",
    "    # As the JSON name is stored in the json_name column, fill other parts of the path with Na if the name of the JSON is present\n",
    "    # If the value is not the name of the value, return the original value\n",
    "    mark = \".json\"\n",
    "\n",
    "    df[['path_2', 'path_3', 'path_4']] =  df[['path_2', 'path_3', 'path_4']].map(lambda x: np.nan if isinstance(x, str) and mark in x else x)\n",
    "\n",
    "    # Unlist the value column (where the JSON info is stored) if it contains a list, otherwise return the original value \n",
    "    df['value'] = df['value'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### string_to_dict()\n",
    "As the JSON files are loaded as strings, they need to be converted to dictionaries to extract the values and be cleaned. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_dict(s):\n",
    "    # Check if the items needed for splitting are present. If not present it does not need to be splitted and the orginal value is returned\n",
    "    if ',' not in s and ':' not in s:\n",
    "        return s\n",
    "    \n",
    "    # Create an empty dictionary\n",
    "    result = {}\n",
    "\n",
    "    # Split the items by comma (split into key-value pairs)\n",
    "    items = s.split(',')\n",
    "\n",
    "    # For each item in the original dictionary\n",
    "    for item in items:\n",
    "        # Check if it contains a key-value pair, if not continue\n",
    "        if ':' not in item:\n",
    "            continue  \n",
    "\n",
    "        # Split the key-value pair into a variable 'key' and a variable 'value'\n",
    "        key, value = item.split(':', 1)  # use maxsplit=1 to avoid unpacking issue\n",
    "\n",
    "        # Try to strip any white spaces from the keys and values \n",
    "        try:\n",
    "            key = eval(key.strip())\n",
    "            value = eval(value.strip())\n",
    "        # If not possible, continue\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        # Save the converted and cleaned dictionary\n",
    "        result[key] = value\n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detect_list()\n",
    "\n",
    "Due to lists in unexpected places in the JSON files and lengthy json paths, we need to identify the positions of lists to later select the correct get() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign list if the data in the original structure is a list\n",
    "    \n",
    "def detect_list(x):\n",
    "    # If the data type of the value is list, 'LIST' is assigned in the '_LIST' columns (see'column_paths())\n",
    "    if isinstance(x, list):\n",
    "        return 'LIST'\n",
    "    # If the value is missing, 'MISSING' is assigned\n",
    "    elif pd.isna(x):\n",
    "        return 'MISSING'\n",
    "    # If the value is 'No data' (there is an empty place holder) 'MISSING' is assigned\n",
    "    elif x == 'No data':\n",
    "            return 'MISSING'\n",
    "    # if the value is not missing and is not a list, 'NO LIST' is assigned\n",
    "    else:\n",
    "        return 'NO LIST'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns_paths()\n",
    "Through this function, we map the json paths to each value by putting the keys in separate columns: colpath_{1,2,3,4,5} (see 'row_column_paths()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_paths(df, col_var, get_var, unlist_var, explode_var):\n",
    "\n",
    "\n",
    "    \n",
    "    # Convert to dictionary if it's a string\n",
    "    df[col_var] = df[col_var].apply(\n",
    "        lambda x: string_to_dict(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    \n",
    "    # Extract nested values using keys\n",
    "    \"\"\"\n",
    "    1. Selects the key obtained in the previous iteration stored in 'get_var'\n",
    "    2. The key put into a get() function which extracts values of a dictionary based on the keys\n",
    "    3. The get() function extracts the values from the (nested) dictionary stored in 'col_var'\n",
    "    4. If the get() function fails, None is returned\n",
    "    5. The steps above are only performed if the value stored in 'col_var' is a dictionary, otherwise None is returned\n",
    "    6. All steps above are executed for each row in the df\n",
    "    \"\"\"\n",
    "    df[unlist_var] = df.apply(lambda row: row[col_var].get(row[get_var], None) if isinstance (row[col_var], dict) else None, axis=1)\n",
    "\n",
    "\n",
    "    # For each each column_path, check if the value contains a list\n",
    "    df[f'{explode_var}_LIST'] = df[unlist_var].apply(detect_list)\n",
    "\n",
    "    df[unlist_var] = df[unlist_var].apply(\n",
    "        lambda x: [x] if not isinstance(x, list) else x)\n",
    "\n",
    "    df = df.explode(unlist_var)\n",
    "\n",
    "    # Unlist data to avoid double nested lists \n",
    "    df[unlist_var] = df[unlist_var].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "    \n",
    "    df[explode_var] = df[unlist_var]\n",
    "\n",
    "    # Explode the list containing the obtained key values into one key per row\n",
    "    df = df.explode(explode_var)\n",
    "\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### row_column_paths()\n",
    "This function prepares the row paths and consequently executes the column_paths function to obtain the paths of keys (stored in individual columns) to reach the lowest value which is where the actual data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_column_paths(df):\n",
    "    # Initialize columns for the row and column paths \n",
    "    df['row_path'] = '' \n",
    "    df['col_path_1'] = ''  \n",
    "    df['col_path_2'] = ''  \n",
    "    df['col_path_3'] = ''\n",
    "    df['col_path_4'] = ''\n",
    "    df['col_path_5'] = ''\n",
    "    df['col_path_6'] = '' \n",
    "    df['col_path_7'] = ''\n",
    "    df['col_path_8'] = ''\n",
    "\n",
    "\n",
    "    # Take all the keys of the dictionaries in the 'value' column and wrap them in a list\n",
    "    df['row_path'] = df['value'].apply(lambda x: list(x.keys()))\n",
    "\n",
    "\n",
    "    # Take the level 1 keys stored in a list and store them in individual rows\n",
    "    df = df.explode('row_path')\n",
    "\n",
    "\n",
    "    # Initiate the list of columns needed to inform the function column_paths()\n",
    "    col_var = ['value', 'col_path_1_values', 'col_path_2_values', 'col_path_3_values', 'col_path_4_values', 'col_path_5_values', 'col_path_6_values', 'col_path_7_values']\n",
    "    get_var = ['row_path', 'col_path_1', 'col_path_2', 'col_path_3', 'col_path_4', 'col_path_5', 'col_path_6', 'col_path_7']\n",
    "    unlist_var = ['col_path_1_values', 'col_path_2_values', 'col_path_3_values', 'col_path_4_values', 'col_path_5_values', 'col_path_6_values', 'col_path_7_values', 'col_path_8_values']\n",
    "    explode_var = ['col_path_1', 'col_path_2', 'col_path_3', 'col_path_4', 'col_path_5', 'col_path_6', 'col_path_7', 'col_path_8']\n",
    "\n",
    "    # Execute the colums_path() function\n",
    "    # zip is necessary due to the large number of variables needed in the column_paths function\n",
    "    for r, g, u, e in zip(col_var, get_var, unlist_var, explode_var):\n",
    "        df = column_paths(df, r, g, u, e)\n",
    "        col_var = unlist_var\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_and_store()\n",
    "This function orders the columns in the DataFrame, resets the index, fills the NA and saves the DataFrame as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_store(df, file_name):\n",
    " \n",
    "    \"\"\"\n",
    "    df: The dataframe that will be cleaned and stored\n",
    "    file_name: The filename of data structure that is being processed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reorder the columns in the df\n",
    "    df = df[['variable', 'value', 'path_1', 'path_2',\n",
    "                  'path_3', 'path_4', 'json_name', 'row_path', \n",
    "                  'col_path_1', 'col_path_1_LIST',\n",
    "                  'col_path_2', 'col_path_2_LIST',\n",
    "                  'col_path_3', 'col_path_3_LIST',\n",
    "                  'col_path_4', 'col_path_4_LIST',\n",
    "                  'col_path_5', 'col_path_5_LIST',\n",
    "                  'col_path_6', 'col_path_6_LIST',\n",
    "                  'col_path_7', 'col_path_7_LIST',\n",
    "                  'col_path_8', 'col_path_8_LIST',\n",
    "                  'data_type']]\n",
    "    \n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # fill na values with 'Missing'\n",
    "    df = df.fillna('Missing')\n",
    "\n",
    "\n",
    "    # Save the dataframe \n",
    "    df.to_csv(f\"{main_path}Facebook/Output/Output_\" + file_name + '.csv', index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### structure_donations()\n",
    "The structure_donations() function executes all functions above and results in a saved DataFrame for each data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_donations(data):\n",
    "\n",
    "    \"\"\"\n",
    "    data: The unprocessed data structure JSON that will be processed\n",
    "    \"\"\"\n",
    "\n",
    "    # Store the path to the data structure\n",
    "    data = Path(data)  \n",
    "    \n",
    "    # Save teh file name of the data structure\n",
    "    file_name = Path(data).stem \n",
    "\n",
    "    # Load JSON file (data structures)\n",
    "    with open(data, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    \n",
    "    # Flatten JSON (handling nested structures)\n",
    "    df = pd.json_normalize(data, max_level=0)\n",
    "    \n",
    "\n",
    "    # Delete user specific information\n",
    "    #df.columns = df.columns.str.replace(r'^[^/]+/', '', regex=True)\n",
    "    \n",
    "    # Extract column names\n",
    "    cols = df.columns.tolist()\n",
    "\n",
    "\n",
    "    # From wide to long df\n",
    "    df = pd.melt(df, value_vars= cols)\n",
    "\n",
    "    # Execute the 'file_paths()' function and store the result in df\n",
    "    df = file_paths(df)\n",
    "    \n",
    "    # Execute the 'row_column_paths()' function and store the result in df\n",
    "    df = row_column_paths(df)\n",
    "    \n",
    "    # Execute the 'process_col_path()' function and store the result in df\n",
    "    df = df.apply(lambda row: process_col_path(row, columns, data_types), axis=1)\n",
    "\n",
    "    # Execute the 'clean_and_store()' function and store the result in df\n",
    "    df = clean_and_store(df, file_name)\n",
    "    \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def id_creation(df):\n",
    "    \n",
    "    df = df[['json_name', 'row_path', 'path_3',\n",
    "                      'col_path_1',\n",
    "                      'col_path_2',\n",
    "                      'col_path_3',\n",
    "                      'col_path_4',\n",
    "                      'col_path_5', \n",
    "                      'col_path_6', \n",
    "                      'col_path_7', \n",
    "                      'col_path_8']]\n",
    "\n",
    "    # Drop rows that are completely identical across all columns\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    df_index = df[(df['row_path'] == 'No data')].index\n",
    "    df.drop(df_index, inplace= True)\n",
    "\n",
    "    df['index'] = df.index\n",
    "    col = df.pop('index') \n",
    "    df.insert(0, 'index', col)  \n",
    "\n",
    "    df['name'] = df['json_name'].str.replace(\".json\", \"\")\n",
    "    col = df.pop('name') \n",
    "    df.insert(2, 'name', col) \n",
    "\n",
    "    last = df.apply(pd.Series.last_valid_index, axis=1)\n",
    "    second = df.shift(-1, axis =1).apply(pd.Series.last_valid_index, axis=1)\n",
    "    third =  df.shift(-2, axis =1).apply(pd.Series.last_valid_index, axis=1)\n",
    "    fourth =  df.shift(-3, axis =1).apply(pd.Series.last_valid_index, axis=1)\n",
    "\n",
    "\n",
    "    df['last'] = last\n",
    "    df['second'] = second\n",
    "    df['third'] =  third\n",
    "    df['fourth'] =  fourth\n",
    "\n",
    "    df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['last']}\"]}\", axis = 1)\n",
    "\n",
    "    \n",
    "    duplicate = list(df[df.duplicated(subset = 'id')]['id'])\n",
    "    print('dup0', len(duplicate))\n",
    "   \n",
    "\n",
    "    df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['second']}\"]}:{x[f\"{x['last']}\"]}\" if x['id'] in duplicate else x['id'] , axis = 1)\n",
    "\n",
    "\n",
    "    df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['third']}\"]}:{x[f\"{x['second']}\"]}:{x[f\"{x['last']}\"]}\" \n",
    "                        if x['id']\n",
    "                         in list(df[df.duplicated(subset = 'id')]['id'])\n",
    "                           else x['id'], axis = 1)\n",
    "    \n",
    "    \n",
    "    df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['fourth']}\"]}:{x[f\"{x['third']}\"]}:{x[f\"{x['second']}\"]}:{x[f\"{x['last']}\"]}\" \n",
    "                        if x['id']\n",
    "                         in list(df[df.duplicated(subset = 'id')]['id']) \n",
    "                         else x['id'] , axis = 1)\n",
    "   \n",
    "    df = df.drop(['last', 'second', 'third', 'fourth', 'index', 'name'], axis = 1)\n",
    "\n",
    "    col = df.pop('id') \n",
    "    df.insert(0, 'id', col)  \n",
    "            \n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute 'structure_donations()': Transform data structures from JSON format to tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/Facebook/Input_test\n"
     ]
    }
   ],
   "source": [
    "input_directory = Path(f'{main_path}Facebook/Input_test')  \n",
    "print(input_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the 'structure_donations()' function for each file (data structure) in the input directory\n",
    "for file in input_directory.iterdir():  \n",
    "    if file.is_file():  \n",
    "        structure_donations(file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all data structures into one schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_822458/2481750660.py:42: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged_df = merged_df.replace('Missing', np.nan)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndf_id = id_creation(merged_df)\\n\\nmerge_cols = ['json_name', 'row_path', 'path_3','col_path_1', 'col_path_2', 'col_path_3',\\n       'col_path_4', 'col_path_5', 'col_path_6', 'col_path_7', 'col_path_8']\\n\\nmerged_df= pd.merge(merged_df, df_id, on = merge_cols, how = 'left')\\n\\ncol = merged_df.pop('id') \\nmerged_df.insert(0, 'id', col) \\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the folder containing CSV files\n",
    "output_path = f\"{main_path}Facebook/Output\"\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = list(Path(output_path).glob(\"*.csv\"))\n",
    "\n",
    "# Load all CSVs into a list of DataFrames\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# browser_cookies.json has cookie names in col_path_1, we should only keep 1 to not unnecessarily duplicate rows\n",
    "pattern = r'\\*{4,}'\n",
    "mask = merged_df['col_path_1'].astype(str).str.contains(pattern, na=False)\n",
    "# Select the first row where mask is True\n",
    "first_idx = merged_df[mask].index.min()\n",
    "merged_df = merged_df[(~mask) | (merged_df.index == first_idx)]\n",
    "\n",
    "\n",
    "# Drop rows that are completely identical across all columns except the 'variable' column as this includes profile names duplicating the same structure\n",
    "#merged_df = merged_df.drop_duplicates(subset=merged_df.columns.difference(['variable']))\n",
    "\n",
    "col_subset = merged_df.columns.tolist()\n",
    "col_subset.remove('value')\n",
    "col_subset.remove('variable')\n",
    "\n",
    "# Drop rows that are completely identical across all columns\n",
    "merged_df = merged_df.drop_duplicates(subset= col_subset)\n",
    "\n",
    "\n",
    "\n",
    "# Message user names cause duplication\n",
    "filtered_df = merged_df[merged_df[\"path_1\"] == \"messages\"].drop_duplicates(subset=merged_df.columns.difference(['variable', 'path_3']))\n",
    "merged_df = merged_df[merged_df[\"path_1\"] != \"messages\"]\n",
    "merged_df = pd.concat([merged_df, filtered_df], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Append rows where col1 does not contain 'messages'\n",
    "merged_df = merged_df.replace('Missing', np.nan)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "df_id = id_creation(merged_df)\n",
    "\n",
    "merge_cols = ['json_name', 'row_path', 'path_3','col_path_1', 'col_path_2', 'col_path_3',\n",
    "       'col_path_4', 'col_path_5', 'col_path_6', 'col_path_7', 'col_path_8']\n",
    "\n",
    "merged_df= pd.merge(merged_df, df_id, on = merge_cols, how = 'left')\n",
    "\n",
    "col = merged_df.pop('id') \n",
    "merged_df.insert(0, 'id', col) \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_col_path = 8\n",
    "def extract_path(df, max_col_path) -> tuple[str, ...]:\n",
    "    path = []\n",
    "    for col in [\"row_path\"] + [f\"col_path_{i}\" for i in range(1, max_col_path)]:\n",
    "        val = df[col]\n",
    "        if pd.notna(val) and str(val) != \"Missing\":\n",
    "            path.append(str(val).strip())\n",
    "    return tuple(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "merged_df['path'] = merged_df.apply(lambda x: extract_path(x, max_col_path), axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up and reduce number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_path(row):\n",
    "    path = row['path'] \n",
    "    for i in range(len(path)):\n",
    "        if row[f\"col_path_{i+1}_LIST\"] == \"LIST\":\n",
    "            var_type = 'list'\n",
    "            list_path = json.dumps(path[:-1])\n",
    "            subfield_path = path[-1]\n",
    "            column_name = subfield_path\n",
    "            subfield_path = json.dumps(subfield_path)\n",
    "        elif row[f\"col_path_{i+1}_LIST\"] == \"NO LIST\":\n",
    "            var_type = 'static'\n",
    "            list_path = np.nan\n",
    "            subfield_path = json.dumps(path)\n",
    "            column_name = path[-1]\n",
    "        elif row[f\"col_path_{i+1}_LIST\"] == \"MISSING\" :\n",
    "            var_type = 'skip'\n",
    "            list_path = np.nan\n",
    "            subfield_path = json.dumps(path)\n",
    "            column_name = path[-1]\n",
    "        return list_path, subfield_path, column_name, var_type\n",
    "\n",
    "\n",
    "merged_df[['list_path', 'subfield_path', 'column_name', 'var_type']] = merged_df.apply(lambda x: pd.Series(list_path(x)), axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df['name'] = merged_df['json_name'].str.replace(\".json\", \"\")\n",
    "\n",
    "id_df = merged_df[['name', 'path']]\n",
    "id_df = id_df.drop_duplicates()\n",
    "id_df['id'] = ''\n",
    "\n",
    "for n in range(1,max_col_path):\n",
    "    duplicates = id_df[id_df.duplicated(subset='id', keep=False)]\n",
    "\n",
    "    for i, row in duplicates.iterrows():\n",
    "        path = list(row['path'])\n",
    "        path_zero = path[0]\n",
    "        path_rest = path[-n:]  \n",
    "        name = row['name'] \n",
    "\n",
    "        if [path_zero] == path_rest:\n",
    "            id_list = [name] + path_rest\n",
    "        else:\n",
    "            id_list = [name]+ [path_zero] + path_rest\n",
    "\n",
    "        new_id = ':'.join(id_list)\n",
    "\n",
    "        id_df.at[i, 'id'] = new_id  \n",
    "\n",
    "merged_df = pd.merge(merged_df, id_df, on = ['name', 'path'], how = 'left')\n",
    "\n",
    "col = merged_df.pop('id') \n",
    "merged_df.insert(0, 'id', col) \n",
    "#merged_df['id']= merged_df['id'].fillna(merged_df['variable'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_822458/1501703322.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dup['duplicate_flag'] = 'Yes'\n"
     ]
    }
   ],
   "source": [
    "dup = merged_df[merged_df.duplicated('id', keep= False)]\n",
    "dup['duplicate_flag'] = 'Yes'\n",
    "dup = dup[['id', 'duplicate_flag']]\n",
    "merged_df = pd.merge(merged_df, dup, on = 'id', how = 'left')\n",
    "merged_df['duplicate_flag'] = merged_df['duplicate_flag'].fillna(value = 'No')\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final merged DataFrame\n",
    "merged_df.to_csv(f\"{main_path}Facebook/Final/Merged_structures_FB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2975, 33)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-donations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
