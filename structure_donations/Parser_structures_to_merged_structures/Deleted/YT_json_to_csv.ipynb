{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Youtube Data Structures into a Schema_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The purpose of his jupyter notebook is to parse the collected data structures in earlier iterations of utilising the data donation tool into a schema_df which can be used to inform future iterations of the data donation tool. Note that there are two notebooks to parse the Youtube data structures in the repository as the Youtube data takeout contains both CSV and JSON files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path  \n",
    "\n",
    "\n",
    "main_path = \"/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating functions\n",
    "### process_col_path()\n",
    "The 'process_col_path()' function checks whether the value in a row of for one of the column paths is actually a datatype and stores this value in a column data_type and replaces the original value with NA. The data types are the lowest level values in the JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types\n",
    "data_types = ['string', 'array', 'number', 'boolean', 'object', 'str', 'int', 'float', 'bool', 'dict', 'list']\n",
    "\n",
    "#Define the column names\n",
    "columns = ['col_path_1','col_path_2','col_path_3','col_path_4', 'col_path_5']\n",
    "\n",
    "\n",
    "\n",
    "# Define the function 'process_col_path()'\n",
    "def process_col_path(row, columns, data_types):\n",
    "\n",
    "    \"\"\"\n",
    "    row: Rows in the dataframe\n",
    "    columns: List of column names of column path columns \n",
    "    data_types: List of the values that are data types\n",
    "    \"\"\"\n",
    "\n",
    "    row['data_type'] = ''\n",
    "    for column in columns:\n",
    "\n",
    "        #If the value stored in the column is found in the list 'data_types', \n",
    "        if row[column] in data_types:\n",
    "            # this value is placed in the column 'data_type'\n",
    "            row['data_type'] = row[column]\n",
    "            # and the original value is replaced with NA\n",
    "            row[column] = np.nan\n",
    "\n",
    "        #If the value is not found in the 'data_types' list, the original value is returned\n",
    "        else:\n",
    "            row[column]\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file_paths()\n",
    "The 'file_paths()' function splits up the paths to where the JSON file is stored in the folder and provides the name of the json file. \n",
    "- 'path_{1,2,3,4}': Column including the names of the {first, second, third, fourth} level folder where the JSON file is stored\n",
    "- 'json_name': Column including the name of the JSON file\n",
    "\n",
    "If the JSON name appears in the 'path_{1,2,3,4}' column, this name is replaced with NA and stored in the column 'json_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_paths(df):\n",
    "    \n",
    "    # Create different columns for each part of the document path\n",
    "    df['path_1'] = df['variable'].str.split('/', n=1).str[0]\n",
    "    df['path_2'] = df['variable'].str.split('/', n=3).str[1]\n",
    "    df['path_3'] = df['variable'].str.split('/', n=3).str[2]\n",
    "    df['path_4'] = df['variable'].str.split('/', n=3).str[3]\n",
    "\n",
    "    print(df['variable'])\n",
    "\n",
    "    # Create a column with the JSON name\n",
    "    df['json_name'] = df['variable'].str.rsplit('/', n=1).str[-1]\n",
    "\n",
    "\n",
    "    # As the JSON name is stored in the json_name column, fill other parts of the path with Na if the name of the JSON is present\n",
    "    # If the value is not the name of the value, return the original value\n",
    "    mark = \".json\"\n",
    "\n",
    "    df[['path_2', 'path_3', 'path_4']] =  df[['path_2', 'path_3', 'path_4']].map(lambda x: np.nan if isinstance(x, str) and mark in x else x)\n",
    "\n",
    "    # Unlist the value column (where the JSON info is stored) if it contains a list, otherwise return the original value \n",
    "    df['value'] = df['value'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detect_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign list if the data in the original structure is a list\n",
    "    \n",
    "def detect_list(x):\n",
    "    # If the data type of the value is list, 'LIST' is assigned in the '_LIST' columns (see'column_paths())\n",
    "    if isinstance(x, list):\n",
    "        return 'LIST'\n",
    "    # If the value is missing, 'MISSING' is assigned\n",
    "    elif pd.isna(x):\n",
    "        return 'MISSING'\n",
    "    # If the value is 'No data' (there is an empty place holder) 'MISSING' is assigned\n",
    "    elif x == 'No data':\n",
    "            return 'MISSING'\n",
    "    # if the value is not missing and is not a list, 'NO LIST' is assigned\n",
    "    else:\n",
    "        return 'NO LIST'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns_paths()\n",
    "Through this function, we map the json paths to each value by putting the keys in separate columns: colpath_{1,2,3,4,5} (see 'row_column_paths()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_paths(df, col_var, get_var, unlist_var, explode_var):\n",
    "\n",
    "        # Extract nested values using keys\n",
    "        \"\"\"\n",
    "        1. Selects the key obtained in the previous iteration stored in 'get_var'\n",
    "        2. The key put into a get() function which extracts values of a dictionary based on the keys\n",
    "        3. The get() function extracts the values from the (nested) dictionary stored in 'col_var'\n",
    "        4. If the get() function fails, None is returned\n",
    "        5. The steps above are only performed if the value stored in 'col_var' is a dictionary, otherwise None is returned\n",
    "        6. All steps above are executed for each row in the df\n",
    "        \"\"\"\n",
    "        df[unlist_var] = df.apply(lambda row: row[col_var].get(row[get_var], None) if isinstance (row[col_var], dict) else None, axis=1)\n",
    "\n",
    "        \n",
    "         # For each each column_path, check if the value contains a list\n",
    "        df[f'{explode_var}_LIST'] = df[unlist_var].apply(detect_list)\n",
    "\n",
    "        df[unlist_var] = df[unlist_var].apply(\n",
    "        lambda x: [x] if not isinstance(x, list) else x)\n",
    "\n",
    "        df = df.explode(unlist_var)\n",
    "        \n",
    "\n",
    "        # Unlist data to avoid double nested lists \n",
    "        df[unlist_var] = df[unlist_var].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "        df[explode_var] = df[unlist_var]\n",
    "\n",
    "         # Explode the list containing the obtained key values into one key per row\n",
    "        df = df.explode(explode_var)\n",
    "\n",
    "    \n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### row_column_paths()\n",
    "This function prepares the row paths and consequently executes the column_paths function to obtain the paths of keys (stored in individual columns) to reach the lowest value which is where the actual data is stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_column_paths(df):\n",
    "    # Initialize the new columns\n",
    "    df['row_path'] = '' \n",
    "    df['col_path_1'] = ''  \n",
    "    df['col_path_2'] = ''  \n",
    "    df['col_path_3'] = ''\n",
    "    df['col_path_4'] = ''\n",
    "    df['col_path_5'] = '' \n",
    "\n",
    "   \n",
    "    # Take all the keys of the dictionaries in the 'value' column and wrap them in a list\n",
    "    df['row_path'] = df['value'].apply(lambda x: list(x.keys()))\n",
    "\n",
    "\n",
    "    \n",
    "    # Take the level 1 keys stored in a list and store them in individual rows\n",
    "    df = df.explode('row_path')\n",
    "  \n",
    "\n",
    "\n",
    "    # Initiate the list of columns needed to inform the function column_paths()\n",
    "    col_var = ['value', 'col_path_1_values', 'col_path_2_values', 'col_path_3_values', 'col_path_4_values']\n",
    "    get_var = ['row_path', 'col_path_1', 'col_path_2', 'col_path_3', 'col_path_4']\n",
    "    unlist_var = ['col_path_1_values', 'col_path_2_values', 'col_path_3_values', 'col_path_4_values', 'col_path_5_values']\n",
    "    explode_var = ['col_path_1', 'col_path_2', 'col_path_3', 'col_path_4', 'col_path_5']\n",
    "\n",
    "    # Execute the colums_path() function\n",
    "    # zip is necessary due to the large number of variables needed in the column_paths function\n",
    "    for r, g, u, e in zip(col_var, get_var, unlist_var, explode_var):\n",
    "        df = column_paths(df, r, g, u, e)\n",
    "        col_var = unlist_var\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_and_store()\n",
    "This function orders the columns in the DataFrame, resets the index, fills the NA and saves the DataFrame as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_store(df, file_name):\n",
    " \n",
    "    # Reorder the columns in the df\n",
    "    df = df[['variable', 'value', 'path_1', 'path_2',\n",
    "        'path_3', 'path_4', 'json_name', 'row_path', \n",
    "        'col_path_1', 'col_path_1_LIST',\n",
    "                     'col_path_2', 'col_path_2_LIST',\n",
    "                     'col_path_3', 'col_path_3_LIST','data_type']]\n",
    "    \n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # fill na values with 'Missing'\n",
    "    df = df.fillna('Missing')\n",
    "\n",
    "    # Save the DataFrame \n",
    "    df.to_csv(f\"{main_path}Youtube/Output/Output_\" + file_name + '.csv', index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### structure_donations()\n",
    "The structure_donations() function executes all functions above and results in a saved DataFrame for each data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_donations(data):\n",
    "\n",
    "    \"\"\"\n",
    "    data: The unprocessed data structure JSON that will be processed\n",
    "    \"\"\"\n",
    "\n",
    "    # Store the path to the data structure\n",
    "    data = Path(data)  \n",
    "    \n",
    "    # Save teh file name of the data structure\n",
    "    file_name = Path(data).stem \n",
    "\n",
    "    # Load JSON file (data structures)\n",
    "    with open(data, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    \n",
    "    # Flatten JSON (handling nested structures)\n",
    "    df = pd.json_normalize(data, max_level=0)\n",
    "    \n",
    "    # Extract column names\n",
    "    cols = df.columns.tolist()\n",
    "\n",
    "\n",
    "    # From wide to long df\n",
    "    df = pd.melt(df, value_vars= cols)\n",
    "\n",
    "    # Execute the 'file_paths()' function and store the result in df\n",
    "    df = file_paths(df)\n",
    "    \n",
    "    # Execute the 'row_column_paths()' function and store the result in df\n",
    "    df = row_column_paths(df)\n",
    "    \n",
    "    # Execute the 'process_col_path()' function and store the result in df\n",
    "    df = df.apply(lambda row: process_col_path(row, columns, data_types), axis=1)\n",
    "\n",
    "    # Execute the 'clean_and_store()' function and store the result in df\n",
    "    df = clean_and_store(df, file_name)\n",
    "    \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute 'structure_donations()': Transform data structures from JSON format to tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/Youtube/Input_test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_directory = Path(f'{main_path}Youtube/Input_test')  \n",
    "print(input_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          historial/historial-de-búsqueda.json\n",
      "1    historial/historial-de-reproducciones.json\n",
      "Name: variable, dtype: object\n",
      "0    geschiedenis/kijkgeschiedenis.json\n",
      "1    geschiedenis/zoekgeschiedenis.json\n",
      "Name: variable, dtype: object\n",
      "0     youtube_takeout/history/watch-history.json\n",
      "1    youtube_takeout/history/search-history.json\n",
      "Name: variable, dtype: object\n",
      "0     istorija/paieškos istorija.json\n",
      "1    istorija/žiūrėjimo istorija.json\n",
      "Name: variable, dtype: object\n",
      "0    history/search-history.json\n",
      "1     history/watch-history.json\n",
      "Name: variable, dtype: object\n",
      "0      istoric/istoricul căutărilor.json\n",
      "1    istoric/istoricul-vizionărilor.json\n",
      "Name: variable, dtype: object\n",
      "0     youtube_takeout/history/watch-history.json\n",
      "1    youtube_takeout/history/search-history.json\n",
      "Name: variable, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Execute the 'structure_donations()' function for each file (data structure) in the input directory\n",
    "for file in input_directory.iterdir():  \n",
    "    if file.is_file():  \n",
    "        structure_donations(file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all data structures into one schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_812641/2804539798.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged_df = merged_df.replace('Missing', np.nan)\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing CSV files\n",
    "output_path = f\"{main_path}Youtube/Output\"\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = list(Path(output_path).glob(\"*.csv\"))\n",
    "\n",
    "# Load all CSVs into a list of DataFrames\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "col_subset = merged_df.columns.tolist()\n",
    "col_subset.remove('value')\n",
    "\n",
    "# Drop rows that are completely identical across all columns\n",
    "merged_df = merged_df.drop_duplicates(subset= col_subset)\n",
    "\n",
    "# Replace 'Missing' with NA\n",
    "merged_df = merged_df.replace('Missing', np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_col_path = 3\n",
    "def extract_path(df, max_col_path) -> tuple[str, ...]:\n",
    "    path = []\n",
    "    for col in [\"row_path\"] + [f\"col_path_{i}\" for i in range(1, max_col_path)]:\n",
    "        val = df[col]\n",
    "        if pd.notna(val) and str(val) != \"Missing\":\n",
    "            path.append(str(val).strip())\n",
    "    return tuple(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df['path'] = merged_df.apply(lambda x: extract_path(x, max_col_path), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up and reduce number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_path(row):\n",
    "    path = row['path'] \n",
    "    for i in range(len(path)):\n",
    "        if row[f\"col_path_{i+1}_LIST\"] == \"LIST\":\n",
    "            var_type = 'list'\n",
    "            list_path = json.dumps(path[:i+2])\n",
    "            subfield_path = path[-1]\n",
    "            column_name = subfield_path\n",
    "            subfield_path = json.dumps(subfield_path)\n",
    "        elif row[f\"col_path_{i+1}_LIST\"] == \"NO LIST\":\n",
    "            var_type = 'static'\n",
    "            list_path = np.nan\n",
    "            subfield_path = json.dumps(path)\n",
    "            column_name = path[-1]\n",
    "        elif row[f\"col_path_{i+1}_LIST\"] == \"MISSING\" :\n",
    "            var_type = 'skip'\n",
    "            list_path = np.nan\n",
    "            subfield_path = json.dumps(path)\n",
    "            column_name = path[-1]\n",
    "        return list_path, subfield_path, column_name, var_type\n",
    "\n",
    "\n",
    "merged_df[['list_path', 'subfield_path', 'column_name', 'var_type']] = merged_df.apply(lambda x: pd.Series(list_path(x)), axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df['name'] = merged_df['json_name'].str.replace(\".json\", \"\")\n",
    "\n",
    "id_df = merged_df[['name', 'path']]\n",
    "id_df = id_df.drop_duplicates()\n",
    "id_df['id'] = ''\n",
    "\n",
    "for n in range(1,max_col_path):\n",
    "    duplicates = id_df[id_df.duplicated(subset='id', keep=False)]\n",
    "\n",
    "    for i, row in duplicates.iterrows():\n",
    "        path = list(row['path'])\n",
    "        path_zero = path[0]\n",
    "        path_rest = path[-n:]  # Varies each loop to attempt uniqueness\n",
    "        name = row['name'] \n",
    "\n",
    "        if [path_zero] == path_rest:\n",
    "            id_list = [name] + path_rest\n",
    "        else:\n",
    "            id_list = [name]+ [path_zero] + path_rest\n",
    "\n",
    "    \n",
    "        new_id = ':'.join(id_list)\n",
    "\n",
    "        id_df.at[i, 'id'] = new_id  # Properly update the DataFrame\n",
    "\n",
    "merged_df = pd.merge(merged_df, id_df, on = ['name', 'path'], how = 'left')\n",
    "\n",
    "col = merged_df.pop('id') \n",
    "merged_df.insert(0, 'id', col) \n",
    "#merged_df['id']= merged_df['id'].fillna(merged_df['variable'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_812641/1499538199.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dup['duplicate_flag'] = 'Yes'\n"
     ]
    }
   ],
   "source": [
    "dup = merged_df[merged_df.duplicated('id', keep= False)]\n",
    "dup['duplicate_flag'] = 'Yes'\n",
    "dup = dup[['id', 'duplicate_flag']]\n",
    "merged_df = pd.merge(merged_df, dup, on = 'id', how = 'left')\n",
    "merged_df['duplicate_flag'] = merged_df['duplicate_flag'].fillna(value = 'No')\n",
    "merged_df = merged_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final merged DataFrame\n",
    "merged_df.to_csv(f\"{main_path}Youtube/Final/Merged_structures_YT.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-donations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
