{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing X/Twitter Data Structures into a Schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_col_path = 8\n",
    "def columns_creator(max_col_path):\n",
    "    col_path = [f\"col_path_{i}\" for i in range(1, max_col_path+1)]\n",
    "    col_var = ['value']+[f\"col_path_{i}_values\" for i in range(1, max_col_path)]\n",
    "    get_var = ['row_path']+ [f\"col_path_{i}\" for i in range(1, max_col_path)]\n",
    "    unlist_var = [f\"col_path_{i}_values\" for i in range(1, max_col_path+1)]\n",
    "    explode_var = col_path\n",
    "    col_path_list = [f\"col_path_{i}_LIST\" for i in range(1, max_col_path+1)]\n",
    "\n",
    "    \n",
    "    return col_path, col_var, get_var, unlist_var, explode_var,  col_path_list\n",
    "\n",
    "col_path, col_var, get_var, unlist_var, explode_var, col_path_list = columns_creator(max_col_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process_col_path()\n",
    "The 'process_col_path()' function checks whether the value in a row of for one of the column paths is actually a datatype and stores this value in a column data_type and replaces the original value with NA. The data types are the lowest level values in the JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types\n",
    "data_types = ['string', 'array', 'number', 'boolean', 'object', 'str', 'int', 'float', 'bool', 'dict', 'list']\n",
    "\n",
    "#Define the column names\n",
    "columns = col_path\n",
    "\n",
    "\n",
    "\n",
    "# Define the function 'process_col_path()'\n",
    "def process_col_path(row, columns, data_types):\n",
    "\n",
    "    \"\"\"\n",
    "    row: Rows in the dataframe\n",
    "    columns: List of column names of column path columns \n",
    "    data_types: List of the values that are data types\n",
    "    \"\"\"\n",
    "\n",
    "    row['data_type'] = ''\n",
    "    for column in columns:\n",
    "\n",
    "        #If the value stored in the column is found in the list 'data_types', \n",
    "        if row[column] in data_types:\n",
    "            # this value is placed in the column 'data_type'\n",
    "            row['data_type'] = row[column]\n",
    "            # and the original value is replaced with NA\n",
    "            row[column] = np.nan\n",
    "\n",
    "        #If the value is not found in the 'data_types' list, the original value is returned\n",
    "        else:\n",
    "            row[column]\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file_paths()\n",
    "The 'file_paths()' function splits up the paths to where the JSON file is stored in the folder and provides the name of the json file. \n",
    "- 'path_{1,2,3,4}': Column including the names of the {first, second, third, fourth} level folder where the JSON file is stored\n",
    "- 'json_name': Column including the name of the JSON file\n",
    "\n",
    "If the JSON name appears in the 'path_{1,2,3,4}' column, this name is replaced with NA and stored in the column 'json_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_paths(df):\n",
    "\n",
    "    # Create different columns for each part of the document path\n",
    "    df['path_1'] = df['variable'].str.split('/', n=1).str[0]\n",
    "    df['path_2'] = df['variable'].str.split('/', n=3).str[1]\n",
    "    df['path_3'] = df['variable'].str.split('/', n=3).str[2]\n",
    "    df['path_4'] = df['variable'].str.split('/', n=3).str[3]\n",
    "\n",
    "    # Create a column with the JSON name\n",
    "    df['json_name'] = df['variable'].str.rsplit('/', n=1).str[-1]\n",
    "\n",
    "\n",
    "    ## As the JSON name is stored in the json_name column, fill other parts of the path with Na if the name of the JSON is present\n",
    "    # If the value is not the name of the value, return the original value\n",
    "    mark = \".json\"\n",
    "\n",
    "    df[['path_2', 'path_3', 'path_4']] =  df[['path_2', 'path_3', 'path_4']].map(lambda x: np.nan if isinstance(x, str) and mark in x else x)\n",
    "\n",
    "    # Unlist the value column (where the JSON info is stored) if it contains a list, otherwise return the original value \n",
    "    #df['value'] = df['value'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### string_to_dict()\n",
    "As the JSON files are loaded as strings, they need to be converted to dictionaries to extract the values and be cleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_dict(s):\n",
    "    # Check if the items needed for splitting are present. If not present it does not need to be splitted and the orginal value is returned\n",
    "    if ',' not in s and ':' not in s:\n",
    "        return s\n",
    "    \n",
    "    # Create an empty dictionary\n",
    "    result = {}\n",
    "\n",
    "    # Split the items by comma (split into key-value pairs)\n",
    "    items = s.split(',')\n",
    "\n",
    "    # For each item in the original dictionary\n",
    "    for item in items:\n",
    "        # Check if it contains a key-value pair, if not continue\n",
    "        if ':' not in item:\n",
    "            continue  \n",
    "\n",
    "        # Split the key-value pair into a variable 'key' and a variable 'value'\n",
    "        key, value = item.split(':', 1)  # use maxsplit=1 to avoid unpacking issue\n",
    "\n",
    "        # Try to strip any white spaces from the keys and values \n",
    "        try:\n",
    "            key = eval(key.strip())\n",
    "            value = eval(value.strip())\n",
    "        # If not possible, continue\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        # Save the converted and cleaned dictionary\n",
    "        result[key] = value\n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detect_list()\n",
    "\n",
    "Due to lists in unexpected places in the JSON files and lengthy json paths, we need to identify the positions of lists to later select the correct get() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign list if the data in the original structure is a list\n",
    "    \n",
    "def detect_list(x):\n",
    "    # If the data type of the value is list, 'LIST' is assigned in the '_LIST' columns (see'column_paths())\n",
    "    if isinstance(x, list):\n",
    "        return 'LIST'\n",
    "    # If the value is missing, 'MISSING' is assigned\n",
    "    elif pd.isna(x):\n",
    "        return 'MISSING'\n",
    "    # If the value is 'No data' (there is an empty place holder) 'MISSING' is assigned\n",
    "    elif x == 'No data':\n",
    "            return 'MISSING'\n",
    "    # if the value is not missing and is not a list, 'NO LIST' is assigned\n",
    "    else:\n",
    "        return 'NO LIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_paths(df, col_var, get_var, unlist_var, explode_var):\n",
    "        \n",
    "        # Convert to dictionary if it's a string\n",
    "        df[col_var] = df[col_var].apply(\n",
    "            lambda x: string_to_dict(x) if isinstance(x, str) else x)\n",
    "        \n",
    "        #df = df.explode(col_var)\n",
    "\n",
    "        df[get_var] = df.apply(\n",
    "        lambda row: list(row[col_var].keys()) if isinstance(row[col_var], dict) else row[col_var], axis=1)\n",
    "       \n",
    "\n",
    "        df = df.explode(get_var)\n",
    "\n",
    "        \n",
    "        # Extract nested values using keys\n",
    "        \"\"\"\n",
    "        1. Selects the key obtained in the previous iteration stored in 'get_var'\n",
    "        2. The key put into a get() function which extracts values of a dictionary based on the keys\n",
    "        3. The get() function extracts the values from the (nested) dictionary stored in 'col_var'\n",
    "        4. If the get() function fails, None is returned\n",
    "        5. The steps above are only performed if the value stored in 'col_var' is a dictionary, otherwise None is returned\n",
    "        6. All steps above are executed for each row in the df\n",
    "        \"\"\"\n",
    "        df[unlist_var] = df.apply(lambda row: row[col_var].get(row[get_var], None) if isinstance (row[col_var], dict) else None, axis=1)\n",
    "\n",
    "        # For each each column_path, check if the value contains a list\n",
    "        df[f'{explode_var}_LIST'] = df[unlist_var].apply(detect_list)\n",
    "\n",
    "        df[unlist_var] = df[unlist_var].apply(\n",
    "        lambda x: [x] if not isinstance(x, list) else x)\n",
    "\n",
    "        df = df.explode(unlist_var)\n",
    "\n",
    "        # Unlist data to avoid double nested lists \n",
    "        df[unlist_var] = df[unlist_var].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "        \n",
    "        df[explode_var] = df[unlist_var]\n",
    "        \n",
    "\n",
    "    \n",
    "        #print(df.columns.tolist())\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### row_column_paths()\n",
    "This function prepares the row paths and consequently executes the column_paths function to obtain the paths of keys (stored in individual columns) to reach the lowest value which is where the actual data is stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def row_column_paths(df, col_var, get_var, unlist_var, explode_var ):\n",
    "    # Initialize the new columns\n",
    "    \n",
    "    df[col_path_list] = np.nan\n",
    "\n",
    "    \n",
    "    df['value'] = df['value'].apply(\n",
    "        lambda x: [x] if not isinstance(x, list) else x)\n",
    "\n",
    "    \n",
    "    \n",
    "    df = df.explode('value')\n",
    "\n",
    "   \n",
    "    \n",
    "    df['value'] = df['value'].apply(\n",
    "            lambda x: string_to_dict(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    \n",
    "\n",
    "    def extract_row_path(val):\n",
    "        if isinstance(val, dict):\n",
    "            return list(val.keys())\n",
    "        elif isinstance(val, list):\n",
    "            return val\n",
    "        else:\n",
    "            return ['no data']\n",
    "\n",
    "    df['row_path'] = df['value'].apply(extract_row_path)\n",
    "\n",
    "  \n",
    "    # Take the level 1 keys stored in a list and store them in individual rows\n",
    "    df = df.explode('row_path')\n",
    "\n",
    "    \n",
    "    for r, g, u, e in zip(col_var, get_var, unlist_var, explode_var):\n",
    "        df = column_paths(df, r, g, u, e)\n",
    "        col_var = unlist_var\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_and_store()\n",
    "This function orders the columns in the DataFrame, resets the index, fills the NA and saves the DataFrame as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_store(df, file_name):\n",
    "\n",
    "    \"\"\"\n",
    "    df: The dataframe that will be cleaned and stored\n",
    "    file_name: The filename of data structure that is being processed\n",
    "    \"\"\"\n",
    "   \n",
    "    # Reorder the columns in the df\n",
    "    df = df.loc[:, ['variable', 'value', 'path_1', 'path_2','path_3', 'path_4',\n",
    "                     'json_name', 'row_path',\n",
    "                     'col_path_1', 'col_path_1_LIST',\n",
    "                     'col_path_2', 'col_path_2_LIST',\n",
    "                     'col_path_3', 'col_path_3_LIST',\n",
    "                     'col_path_4', 'col_path_4_LIST',\n",
    "                     'col_path_5', 'col_path_5_LIST',\n",
    "                     'col_path_6', 'col_path_6_LIST',\n",
    "                     'col_path_7', 'col_path_7_LIST',\n",
    "                     'col_path_8', 'col_path_8_LIST',\n",
    "                     'data_type']]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # fill na values with 'Missing'\n",
    "    df = df.fillna('Missing')\n",
    "\n",
    "    col_subset = df.columns.tolist()\n",
    "    col_subset.remove('value')\n",
    "\n",
    "    #   Drop rows that are completely identical across all columns\n",
    "    df = df.drop_duplicates(subset= col_subset)\n",
    "    df = df[df[\"data_type\"] != \"array\"]\n",
    "     # Save the DataFrame \n",
    "    df.to_csv(f\"{main_path}Twitter/Output/Output_\" + file_name + '.csv', index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### structure_donations()\n",
    "The structure_donations() function executes all functions above and results in a saved DataFrame for each data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_donations(data):\n",
    "\n",
    "    \"\"\"\n",
    "    data: The unprocessed data structure JSON that will be processed\n",
    "    \"\"\"\n",
    "\n",
    "    # Store the path to the data structure\n",
    "    data = Path(data)  \n",
    "    \n",
    "    # Save teh file name of the data structure\n",
    "    file_name = Path(data).stem \n",
    "\n",
    "    # Load JSON file (data structures)\n",
    "    with open(data, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "\n",
    "    \n",
    "    rows = [{'variable': k, 'value': v} for k, v in data.items()]\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    \n",
    "    # Execute the 'file_paths()' function and store the result in df\n",
    "    df = file_paths(df)\n",
    "    \n",
    "    # Execute the 'row_column_paths()' function and store the result in df\n",
    "    #df = row_column_paths(df)\n",
    "    df = row_column_paths(df, col_var, get_var, unlist_var, explode_var )\n",
    "    \n",
    "    # Execute the 'process_col_path()' function and store the result in df\n",
    "    df = df.apply(lambda row: process_col_path(row, columns, data_types), axis=1)\n",
    "\n",
    "    # Execute the 'clean_and_store()' function and store the result in df\n",
    "    df = clean_and_store(df, file_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### col_path_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_path_counter(df):\n",
    "    columns = list(df.columns)\n",
    "\n",
    "    n_col_paths= []\n",
    "    col_paths = []\n",
    "\n",
    "    for s in columns:\n",
    "        match = re.search(r'col_path_(\\d+)$', s)\n",
    "        print(match)\n",
    "        if match:\n",
    "            n_col_paths.append(int(match.group(1)))\n",
    "            col_paths.append(match)\n",
    "    max_col_path = max(n_col_paths)\n",
    "    \n",
    "    return max_col_path, col_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path(df, max_col_path) -> tuple[str, ...]:\n",
    "    path = []\n",
    "    for col in [\"row_path\"] + [f\"col_path_{i}\" for i in range(1, max_col_path)]:\n",
    "        val = df[col]\n",
    "        if pd.notna(val) and str(val) != \"Missing\":\n",
    "            path.append(str(val).strip())\n",
    "    return tuple(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def id_creation(df):\n",
    "    \n",
    "    c = col_path_counter(df)\n",
    "    print(c)\n",
    "    \n",
    "\n",
    "    cols = ['json_name', 'row_path'] + col_path\n",
    "    df = df[cols]\n",
    "\n",
    "    # Drop rows that are completely identical across all columns\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    print(df[df.duplicated()])\n",
    "    \n",
    "    df_index = df[(df['row_path'] == 'No data')].index\n",
    "    df.drop(df_index, inplace= True)\n",
    "\n",
    "    df['index'] = df.index\n",
    "    col = df.pop('index') \n",
    "    df.insert(0, 'index', col)  \n",
    "\n",
    "    df['name'] = df['json_name'].str.replace(\".js\", \"\")\n",
    "    col = df.pop('name') \n",
    "    df.insert(2, 'name', col) \n",
    "\n",
    "    last = df.apply(pd.Series.last_valid_index, axis=1)\n",
    "    second = df.shift(-1, axis =1).apply(pd.Series.last_valid_index, axis=1)\n",
    "    third =  df.shift(-2, axis =1).apply(pd.Series.last_valid_index, axis=1)\n",
    "    fourth = df.shift(-3, axis =1).apply(pd.Series.last_valid_index, axis=1)\n",
    "    fifth =  df.shift(-4, axis =1).apply(pd.Series.last_valid_index, axis=1)\n",
    "\n",
    "\n",
    "    df['last'] = last\n",
    "    df['second'] = second\n",
    "    df['third'] =  third\n",
    "    df['fourth'] =  fourth\n",
    "    df['fifth'] =  fifth\n",
    "\n",
    "\n",
    "\n",
    "    df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['last']}\"]}\", axis = 1)\n",
    "\n",
    "    \n",
    "    duplicate = list(df[df.duplicated(subset = 'id')]['id'])\n",
    "\n",
    "   \n",
    "\n",
    "    df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['second']}\"]}:{x[f\"{x['last']}\"]}\" if x['id'] in duplicate else x['id'] , axis = 1)\n",
    "\n",
    "\n",
    "    df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['third']}\"]}:{x[f\"{x['second']}\"]}:{x[f\"{x['last']}\"]}\" \n",
    "                        if x['id']\n",
    "                         in list(df[df.duplicated(subset = 'id')]['id'])\n",
    "                           else x['id'], axis = 1)\n",
    "    \n",
    "    \n",
    "    df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['fourth']}\"]}:{x[f\"{x['third']}\"]}:{x[f\"{x['second']}\"]}:{x[f\"{x['last']}\"]}\" \n",
    "                        if x['id']\n",
    "                         in list(df[df.duplicated(subset = 'id')]['id']) \n",
    "                         else x['id'] , axis = 1)\n",
    "    \n",
    "    df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['fifth']}\"]}:{x[f\"{x['fourth']}\"]}:{x[f\"{x['third']}\"]}:{x[f\"{x['second']}\"]}:{x[f\"{x['last']}\"]}\" \n",
    "                        if x['id']\n",
    "                         in list(df[df.duplicated(subset = 'id')]['id']) \n",
    "                         else x['id'] , axis = 1)\n",
    "   \n",
    "    df = df.drop(['last', 'second', 'third', 'fourth', 'fifth', 'index', 'name'], axis = 1)\n",
    "\n",
    "    col = df.pop('id') \n",
    "    df.insert(0, 'id', col)  \n",
    "            \n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute 'structure_donations()': Transform data structures from JSON format to tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/Twitter/Input_test\n"
     ]
    }
   ],
   "source": [
    "# Specify the input directory\n",
    "input_directory = Path(f'{main_path}Twitter/Input_test')  \n",
    "print(input_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the 'structure_donations()' function for each file (data structure) in the input directory\n",
    "for file in input_directory.iterdir():  \n",
    "    if file.is_file():  \n",
    "        structure_donations(file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all data structures into one schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17873/3228039524.py:28: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged_df = merged_df.replace('Missing', np.nan)\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing CSV files\n",
    "output_path = f\"{main_path}Twitter/Output\"\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = list(Path(output_path).glob(\"*.csv\"))\n",
    "\n",
    "# Load all CSVs into a list of DataFrames\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "# Concatenate all dataframes\n",
    "merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "col_subset = merged_df.columns.tolist()\n",
    "col_subset.remove('value')\n",
    "\n",
    "# Drop rows that are completely identical across all columns\n",
    "merged_df = merged_df.drop_duplicates(subset= col_subset)\n",
    "\n",
    "\n",
    "# Filter where col1 contains 'messages', then drop duplicates based on col2\n",
    "df_filtered = merged_df[merged_df[\"path_1\"] == \"messages\"].drop_duplicates(subset=\"path_2\")\n",
    "# Append rows where col1 does not contain 'messages'\n",
    "merged_df = pd.concat([df_filtered, merged_df[merged_df[\"path_1\"] != \"messages\"]], ignore_index=True)\n",
    "\n",
    "\n",
    "merged_df = merged_df.replace('Missing', np.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 10), match='col_path_1'>\n",
      "None\n",
      "<re.Match object; span=(0, 10), match='col_path_2'>\n",
      "None\n",
      "<re.Match object; span=(0, 10), match='col_path_3'>\n",
      "None\n",
      "<re.Match object; span=(0, 10), match='col_path_4'>\n",
      "None\n",
      "<re.Match object; span=(0, 10), match='col_path_5'>\n",
      "None\n",
      "<re.Match object; span=(0, 10), match='col_path_6'>\n",
      "None\n",
      "<re.Match object; span=(0, 10), match='col_path_7'>\n",
      "None\n",
      "<re.Match object; span=(0, 10), match='col_path_8'>\n",
      "None\n",
      "None\n",
      "None\n",
      "(8, [<re.Match object; span=(0, 10), match='col_path_1'>, <re.Match object; span=(0, 10), match='col_path_2'>, <re.Match object; span=(0, 10), match='col_path_3'>, <re.Match object; span=(0, 10), match='col_path_4'>, <re.Match object; span=(0, 10), match='col_path_5'>, <re.Match object; span=(0, 10), match='col_path_6'>, <re.Match object; span=(0, 10), match='col_path_7'>, <re.Match object; span=(0, 10), match='col_path_8'>])\n",
      "Empty DataFrame\n",
      "Columns: [json_name, row_path, col_path_1, col_path_2, col_path_3, col_path_4, col_path_5, col_path_6, col_path_7, col_path_8]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "merged_df['path'] = merged_df.apply(lambda x: extract_path(x, max_col_path), axis = 1)\n",
    "\n",
    "df_id = id_creation(merged_df)\n",
    "\n",
    "merge_cols = ['json_name', 'row_path'] + col_path\n",
    "\n",
    "merged_df = pd.merge(merged_df, df_id, on = merge_cols, how = 'left')\n",
    "\n",
    "merged_df['name'] = merged_df['json_name'].str.replace(\".js\", \"\")\n",
    "merged_df['id']= merged_df['id'].fillna(merged_df['name'])\n",
    "merged_df= merged_df.drop('name', axis = 1)\n",
    "\n",
    "col = merged_df.pop('id') \n",
    "merged_df.insert(0, 'id', col) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up and reduce number of columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_path(df):\n",
    "    path = df['path'] \n",
    "    for i in range(len(path)):\n",
    "        if df[f\"col_path_{i+1}_LIST\"] == \"LIST\":\n",
    "            var_type = 'list'\n",
    "            list_path = '.'.join(path[: i])\n",
    "            subfield_path = ''.join(path[i:])\n",
    "            column_name = subfield_path\n",
    "        else:\n",
    "            var_type = 'static'\n",
    "            list_path = np.nan\n",
    "            subfield_path = '.'.join(path)\n",
    "            column_name = ''.join(path[i:])\n",
    "    return list_path, subfield_path, column_name, var_type\n",
    "\n",
    "\n",
    "merged_df[['list_path', 'subfield_path', 'column_name', 'var_type']] = merged_df.apply(lambda x: pd.Series(list_path(x)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'column_name', 'variable', 'json_name', 'path_1', 'path_2', 'path', 'list_path', 'subfield_path', 'var_type', 'data_type', 'row_path', 'col_path_1', 'col_path_2', 'col_path_3', 'col_path_4', 'col_path_5', 'col_path_6', 'col_path_7', 'col_path_8']\n"
     ]
    }
   ],
   "source": [
    "merged_df.columns\n",
    "\n",
    "keep_columns = ['id', 'column_name',  'variable',\n",
    "                 'json_name', 'path_1', 'path_2', \n",
    "                 'path', 'list_path', 'subfield_path', \n",
    "                 'var_type', 'data_type','row_path'] + col_path\n",
    "\n",
    "print(keep_columns)\n",
    "\n",
    "merged_df = merged_df[keep_columns ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final merged DataFrame\n",
    "merged_df.to_csv(f\"{main_path}Twitter/Final/Merged_structures_X.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-donations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
