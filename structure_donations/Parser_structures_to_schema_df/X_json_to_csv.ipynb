{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing X/Twitter Data Structures into a Schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating functions\n",
    "### process_col_path()\n",
    "The 'process_col_path()' function checks whether the value in a row of for one of the column paths is actually a datatype and stores this value in a column data_type and replaces the original value with NA. The data types are the lowest level values in the JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types\n",
    "data_types = ['string', 'array', 'number', 'boolean', 'object', 'str', 'int', 'float', 'bool', 'dict', 'list']\n",
    "\n",
    "#Define the column names\n",
    "columns = ['col_path_1','col_path_2','col_path_3','col_path_4', 'col_path_5', 'col_path_6', 'col_path_7', 'col_path_8']\n",
    "\n",
    "\n",
    "\n",
    "# Define the function 'process_col_path()'\n",
    "def process_col_path(row, columns, data_types):\n",
    "\n",
    "    \"\"\"\n",
    "    row: Rows in the dataframe\n",
    "    columns: List of column names of column path columns \n",
    "    data_types: List of the values that are data types\n",
    "    \"\"\"\n",
    "\n",
    "    row['data_type'] = ''\n",
    "    for column in columns:\n",
    "\n",
    "        #If the value stored in the column is found in the list 'data_types', \n",
    "        if row[column] in data_types:\n",
    "            # this value is placed in the column 'data_type'\n",
    "            row['data_type'] = row[column]\n",
    "            # and the original value is replaced with NA\n",
    "            row[column] = np.nan\n",
    "\n",
    "        #If the value is not found in the 'data_types' list, the original value is returned\n",
    "        else:\n",
    "            row[column]\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file_paths()\n",
    "The 'file_paths()' function splits up the paths to where the JSON file is stored in the folder and provides the name of the json file. \n",
    "- 'path_{1,2,3,4}': Column including the names of the {first, second, third, fourth} level folder where the JSON file is stored\n",
    "- 'json_name': Column including the name of the JSON file\n",
    "\n",
    "If the JSON name appears in the 'path_{1,2,3,4}' column, this name is replaced with NA and stored in the column 'json_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_paths(df):\n",
    "\n",
    "    # Create different columns for each part of the document path\n",
    "    df['path_1'] = df['variable'].str.split('/', n=1).str[0]\n",
    "    df['path_2'] = df['variable'].str.split('/', n=3).str[1]\n",
    "    df['path_3'] = df['variable'].str.split('/', n=3).str[2]\n",
    "    df['path_4'] = df['variable'].str.split('/', n=3).str[3]\n",
    "\n",
    "    # Create a column with the JSON name\n",
    "    df['json_name'] = df['variable'].str.rsplit('/', n=1).str[-1]\n",
    "\n",
    "\n",
    "    ## As the JSON name is stored in the json_name column, fill other parts of the path with Na if the name of the JSON is present\n",
    "    # If the value is not the name of the value, return the original value\n",
    "    mark = \".json\"\n",
    "\n",
    "    df[['path_2', 'path_3', 'path_4']] =  df[['path_2', 'path_3', 'path_4']].map(lambda x: np.nan if isinstance(x, str) and mark in x else x)\n",
    "\n",
    "    # Unlist the value column (where the JSON info is stored) if it contains a list, otherwise return the original value \n",
    "    #df['value'] = df['value'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### string_to_dict()\n",
    "As the JSON files are loaded as strings, they need to be converted to dictionaries to extract the values and be cleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_dict(s):\n",
    "    # Check if the items needed for splitting are present. If not present it does not need to be splitted and the orginal value is returned\n",
    "    if ',' not in s and ':' not in s:\n",
    "        return s\n",
    "    \n",
    "    # Create an empty dictionary\n",
    "    result = {}\n",
    "\n",
    "    # Split the items by comma (split into key-value pairs)\n",
    "    items = s.split(',')\n",
    "\n",
    "    # For each item in the original dictionary\n",
    "    for item in items:\n",
    "        # Check if it contains a key-value pair, if not continue\n",
    "        if ':' not in item:\n",
    "            continue  \n",
    "\n",
    "        # Split the key-value pair into a variable 'key' and a variable 'value'\n",
    "        key, value = item.split(':', 1)  # use maxsplit=1 to avoid unpacking issue\n",
    "\n",
    "        # Try to strip any white spaces from the keys and values \n",
    "        try:\n",
    "            key = eval(key.strip())\n",
    "            value = eval(value.strip())\n",
    "        # If not possible, continue\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        # Save the converted and cleaned dictionary\n",
    "        result[key] = value\n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detect_list()\n",
    "\n",
    "Due to lists in unexpected places in the JSON files and lengthy json paths, we need to identify the positions of lists to later select the correct get() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign list if the data in the original structure is a list\n",
    "    \n",
    "def detect_list(x):\n",
    "    # If the data type of the value is list, 'LIST' is assigned in the '_LIST' columns (see'column_paths())\n",
    "    if isinstance(x, list):\n",
    "        return 'LIST'\n",
    "    # If the value is missing, 'MISSING' is assigned\n",
    "    elif pd.isna(x):\n",
    "        return 'MISSING'\n",
    "    # If the value is 'No data' (there is an empty place holder) 'MISSING' is assigned\n",
    "    elif x == 'No data':\n",
    "            return 'MISSING'\n",
    "    # if the value is not missing and is not a list, 'NO LIST' is assigned\n",
    "    else:\n",
    "        return 'NO LIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_paths(df, col_var, get_var, unlist_var, explode_var):\n",
    "        \n",
    "        # Convert to dictionary if it's a string\n",
    "        df[col_var] = df[col_var].apply(\n",
    "            lambda x: string_to_dict(x) if isinstance(x, str) else x)\n",
    "        \n",
    "        #df = df.explode(col_var)\n",
    "\n",
    "        df[get_var] = df.apply(\n",
    "        lambda row: list(row[col_var].keys()) if isinstance(row[col_var], dict) else row[col_var], axis=1)\n",
    "       \n",
    "\n",
    "        df = df.explode(get_var)\n",
    "\n",
    "        \n",
    "        # Extract nested values using keys\n",
    "        \"\"\"\n",
    "        1. Selects the key obtained in the previous iteration stored in 'get_var'\n",
    "        2. The key put into a get() function which extracts values of a dictionary based on the keys\n",
    "        3. The get() function extracts the values from the (nested) dictionary stored in 'col_var'\n",
    "        4. If the get() function fails, None is returned\n",
    "        5. The steps above are only performed if the value stored in 'col_var' is a dictionary, otherwise None is returned\n",
    "        6. All steps above are executed for each row in the df\n",
    "        \"\"\"\n",
    "        df[unlist_var] = df.apply(lambda row: row[col_var].get(row[get_var], None) if isinstance (row[col_var], dict) else None, axis=1)\n",
    "\n",
    "        # For each each column_path, check if the value contains a list\n",
    "        df[f'{explode_var}_LIST'] = df[unlist_var].apply(detect_list)\n",
    "\n",
    "        df[unlist_var] = df[unlist_var].apply(\n",
    "        lambda x: [x] if not isinstance(x, list) else x)\n",
    "\n",
    "        df = df.explode(unlist_var)\n",
    "\n",
    "        # Unlist data to avoid double nested lists \n",
    "        df[unlist_var] = df[unlist_var].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "        \n",
    "        df[explode_var] = df[unlist_var]\n",
    "        \n",
    "\n",
    "    \n",
    "        #print(df.columns.tolist())\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### row_column_paths()\n",
    "This function prepares the row paths and consequently executes the column_paths function to obtain the paths of keys (stored in individual columns) to reach the lowest value which is where the actual data is stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def row_column_paths(df):\n",
    "    # Initialize the new columns\n",
    "    df['row_path'] = '' \n",
    "    df['col_path_1'] = ''  \n",
    "    df['col_path_2'] = ''  \n",
    "    df['col_path_3'] = ''\n",
    "    df['col_path_4'] = ''\n",
    "    df['col_path_6'] = '' \n",
    "    df['col_path_7'] = ''\n",
    "    df['col_path_8'] = ''\n",
    "\n",
    "    \n",
    "    df['value'] = df['value'].apply(\n",
    "        lambda x: [x] if not isinstance(x, list) else x)\n",
    "\n",
    "    \n",
    "    \n",
    "    df = df.explode('value')\n",
    "\n",
    "   \n",
    "    \n",
    "    df['value'] = df['value'].apply(\n",
    "            lambda x: string_to_dict(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    \n",
    "\n",
    "    def extract_row_path(val):\n",
    "        if isinstance(val, dict):\n",
    "            return list(val.keys())\n",
    "        elif isinstance(val, list):\n",
    "            return val\n",
    "        else:\n",
    "            return ['no data']\n",
    "\n",
    "    df['row_path'] = df['value'].apply(extract_row_path)\n",
    "\n",
    "    #df['row_path'] = df['row_path'].astype('str').apply(eval).str[0]\n",
    "  \n",
    "    # Take the level 1 keys stored in a list and store them in individual rows\n",
    "    df = df.explode('row_path')\n",
    "\n",
    "    # Initiate the list of columns needed to inform the function column_paths()\n",
    "    col_var = ['value', 'col_path_1_values', 'col_path_2_values', 'col_path_3_values', 'col_path_4_values', 'col_path_5_values', 'col_path_6_values', 'col_path_7_values']\n",
    "    get_var = ['row_path', 'col_path_1', 'col_path_2', 'col_path_3', 'col_path_4',  'col_path_5',  'col_path_6',  'col_path_7']\n",
    "    unlist_var = ['col_path_1_values', 'col_path_2_values', 'col_path_3_values', 'col_path_4_values', 'col_path_5_values', 'col_path_6_values', 'col_path_7_values', 'col_path_8_values']\n",
    "    explode_var = ['col_path_1', 'col_path_2', 'col_path_3', 'col_path_4', 'col_path_5', 'col_path_6', 'col_path_7', 'col_path_8']\n",
    "\n",
    "    for r, g, u, e in zip(col_var, get_var, unlist_var, explode_var):\n",
    "        df = column_paths(df, r, g, u, e)\n",
    "        col_var = unlist_var\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_and_store()\n",
    "This function orders the columns in the DataFrame, resets the index, fills the NA and saves the DataFrame as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_store(df, file_name):\n",
    "\n",
    "    \"\"\"\n",
    "    df: The dataframe that will be cleaned and stored\n",
    "    file_name: The filename of data structure that is being processed\n",
    "    \"\"\"\n",
    "   \n",
    "    # Reorder the columns in the df\n",
    "    df = df.loc[:, ['variable', 'value', 'path_1', 'path_2','path_3', 'path_4',\n",
    "                     'json_name', 'row_path',\n",
    "                     'col_path_1', 'col_path_1_LIST',\n",
    "                     'col_path_2', 'col_path_2_LIST',\n",
    "                     'col_path_3', 'col_path_3_LIST',\n",
    "                     'col_path_4', 'col_path_4_LIST',\n",
    "                     'col_path_5', 'col_path_5_LIST',\n",
    "                     'col_path_6', 'col_path_6_LIST',\n",
    "                     'col_path_7', 'col_path_7_LIST',\n",
    "                     'col_path_8', 'col_path_8_LIST',\n",
    "                     'data_type']]\n",
    "    \n",
    "    \"\"\"\n",
    "    # Reorder the columns in the df\n",
    "    df = df.loc[:, ['variable', 'value', 'path_1', 'path_2','path_3', 'path_4',\n",
    "                     'json_name', 'row_path',\n",
    "                     'col_path_1', 'col_path_1_LIST',\n",
    "                     'col_path_2', 'col_path_2_LIST',\n",
    "                     'col_path_3', 'col_path_3_LIST',\n",
    "                     'col_path_4', 'col_path_4_LIST',\n",
    "                     'col_path_5', 'col_path_5_LIST',\n",
    "                     'col_path_6', 'col_path_6_LIST',\n",
    "                     'col_path_7', 'col_path_7_LIST',\n",
    "                     'col_path_8', 'col_path_8_LIST',\n",
    "                     'data_type', 'col_path_1_values',\n",
    "                     'col_path_2_values','col_path_3_values',\n",
    "                     'col_path_4_values', 'col_path_5_values',\n",
    "                     'col_path_6_values', 'col_path_7_values',\n",
    "                     'col_path_8_values']]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # fill na values with 'Missing'\n",
    "    df = df.fillna('Missing')\n",
    "\n",
    "    col_subset = df.columns.tolist()\n",
    "    col_subset.remove('value')\n",
    "\n",
    "    #   Drop rows that are completely identical across all columns\n",
    "    df = df.drop_duplicates(subset= col_subset)\n",
    "   \n",
    "     # Save the DataFrame \n",
    "    df.to_csv(f\"{main_path}Twitter/Output/Output_\" + file_name + '.csv', index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_creation(df):\n",
    "    print(df.where(df.map(lambda x: x == np.nan)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### structure_donations()\n",
    "The structure_donations() function executes all functions above and results in a saved DataFrame for each data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_donations(data):\n",
    "\n",
    "    \"\"\"\n",
    "    data: The unprocessed data structure JSON that will be processed\n",
    "    \"\"\"\n",
    "\n",
    "    # Store the path to the data structure\n",
    "    data = Path(data)  \n",
    "    \n",
    "    # Save teh file name of the data structure\n",
    "    file_name = Path(data).stem \n",
    "\n",
    "    # Load JSON file (data structures)\n",
    "    with open(data, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "\n",
    "    \n",
    "    rows = [{'variable': k, 'value': v} for k, v in data.items()]\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    \n",
    "    # Execute the 'file_paths()' function and store the result in df\n",
    "    df = file_paths(df)\n",
    "    \n",
    "    # Execute the 'row_column_paths()' function and store the result in df\n",
    "    df = row_column_paths(df)\n",
    "    \n",
    "    # Execute the 'process_col_path()' function and store the result in df\n",
    "    df = df.apply(lambda row: process_col_path(row, columns, data_types), axis=1)\n",
    "\n",
    "    # Execute the 'clean_and_store()' function and store the result in df\n",
    "    df = clean_and_store(df, file_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def id_creation(df):\n",
    "    \n",
    "    df = df[['json_name', 'row_path',\n",
    "                      'col_path_1',\n",
    "                      'col_path_2',\n",
    "                      'col_path_3',\n",
    "                      'col_path_4',\n",
    "                      'col_path_5', \n",
    "                      'col_path_6', \n",
    "                      'col_path_7', \n",
    "                      'col_path_8']]\n",
    "\n",
    "    # Drop rows that are completely identical across all columns\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    df_index = df[(df['row_path'] == 'No data')].index\n",
    "    df.drop(df_index, inplace= True)\n",
    "\n",
    "    df['index'] = df.index\n",
    "    col = df.pop('index') \n",
    "    df.insert(0, 'index', col)  \n",
    "\n",
    "    df['name'] = df['json_name'].str.replace(\".js\", \"\")\n",
    "    col = df.pop('name') \n",
    "    df.insert(2, 'name', col) \n",
    "\n",
    "    last = df.apply(pd.Series.last_valid_index, axis=1)\n",
    "    second = df.shift(-1, axis =1).apply(pd.Series.last_valid_index, axis=1)\n",
    "    third =  df.shift(-2, axis =1).apply(pd.Series.last_valid_index, axis=1)\n",
    "    fourth =  df.shift(-3, axis =1).apply(pd.Series.last_valid_index, axis=1)\n",
    "\n",
    "\n",
    "    df['last'] = last\n",
    "    df['second'] = second\n",
    "    df['third'] =  third\n",
    "    df['fourth'] =  fourth\n",
    "\n",
    "    df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['last']}\"]}\", axis = 1)\n",
    "\n",
    "    \n",
    "    duplicate = list(df[df.duplicated(subset = 'id')]['id'])\n",
    "    print('dup0', len(duplicate))\n",
    "   \n",
    "\n",
    "    df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['second']}\"]}:{x[f\"{x['last']}\"]}\" if x['id'] in duplicate else x['id'] , axis = 1)\n",
    "\n",
    "\n",
    "    df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['third']}\"]}:{x[f\"{x['second']}\"]}:{x[f\"{x['last']}\"]}\" \n",
    "                        if x['id']\n",
    "                         in list(df[df.duplicated(subset = 'id')]['id'])\n",
    "                           else x['id'], axis = 1)\n",
    "    \n",
    "    \n",
    "    df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['fourth']}\"]}:{x[f\"{x['third']}\"]}:{x[f\"{x['second']}\"]}:{x[f\"{x['last']}\"]}\" \n",
    "                        if x['id']\n",
    "                         in list(df[df.duplicated(subset = 'id')]['id']) \n",
    "                         else x['id'] , axis = 1)\n",
    "   \n",
    "    df = df.drop(['last', 'second', 'third', 'fourth', 'index', 'name'], axis = 1)\n",
    "\n",
    "    col = df.pop('id') \n",
    "    df.insert(0, 'id', col)  \n",
    "            \n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute 'structure_donations()': Transform data structures from JSON format to tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/Twitter/Input_test\n"
     ]
    }
   ],
   "source": [
    "# Specify the input directory\n",
    "input_directory = Path(f'{main_path}Twitter/Input_test')  \n",
    "print(input_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the 'structure_donations()' function for each file (data structure) in the input directory\n",
    "for file in input_directory.iterdir():  \n",
    "    if file.is_file():  \n",
    "        structure_donations(file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all data structures into one schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_843124/4228252219.py:28: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_final = df_final.replace('Missing', np.nan)\n",
      "/tmp/ipykernel_843124/3672645115.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(df_index, inplace= True)\n",
      "/tmp/ipykernel_843124/3672645115.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['index'] = df.index\n",
      "/tmp/ipykernel_843124/3672645115.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['name'] = df['json_name'].str.replace(\".js\", \"\")\n",
      "/tmp/ipykernel_843124/3672645115.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['last'] = last\n",
      "/tmp/ipykernel_843124/3672645115.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['second'] = second\n",
      "/tmp/ipykernel_843124/3672645115.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['third'] =  third\n",
      "/tmp/ipykernel_843124/3672645115.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['fourth'] =  fourth\n",
      "/tmp/ipykernel_843124/3672645115.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['last']}\"]}\", axis = 1)\n",
      "/tmp/ipykernel_843124/3672645115.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['second']}\"]}:{x[f\"{x['last']}\"]}\" if x['id'] in duplicate else x['id'] , axis = 1)\n",
      "/tmp/ipykernel_843124/3672645115.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['third']}\"]}:{x[f\"{x['second']}\"]}:{x[f\"{x['last']}\"]}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dup0 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_843124/3672645115.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['id'] = df.apply(lambda x: f\"{x['name']}:{x[f\"{x['fourth']}\"]}:{x[f\"{x['third']}\"]}:{x[f\"{x['second']}\"]}:{x[f\"{x['last']}\"]}\"\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing CSV files\n",
    "output_path = f\"{main_path}Twitter/Output\"\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = list(Path(output_path).glob(\"*.csv\"))\n",
    "\n",
    "# Load all CSVs into a list of DataFrames\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "# Concatenate all dataframes\n",
    "merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "col_subset = merged_df.columns.tolist()\n",
    "col_subset.remove('value')\n",
    "\n",
    "# Drop rows that are completely identical across all columns\n",
    "merged_df = merged_df.drop_duplicates(subset= col_subset)\n",
    "\n",
    "\n",
    "# Filter where col1 contains 'messages', then drop duplicates based on col2\n",
    "df_filtered = merged_df[merged_df[\"path_1\"] == \"messages\"].drop_duplicates(subset=\"path_2\")\n",
    "# Append rows where col1 does not contain 'messages'\n",
    "merged_df = pd.concat([df_filtered, merged_df[merged_df[\"path_1\"] != \"messages\"]], ignore_index=True)\n",
    "\n",
    "\n",
    "merged_df = merged_df.replace('Missing', np.nan)\n",
    "\n",
    "df_id = id_creation(merged_df)\n",
    "\n",
    "merge_cols = ['json_name', 'row_path', 'col_path_1', 'col_path_2', 'col_path_3',\n",
    "       'col_path_4', 'col_path_5', 'col_path_6', 'col_path_7', 'col_path_8']\n",
    "\n",
    "merged_df = pd.merge(merged_df, df_id, on = merge_cols, how = 'left')\n",
    "\n",
    "merged_df['name'] = merged_df['json_name'].str.replace(\".js\", \"\")\n",
    "merged_df['id']= merged_df['id'].fillna(df_final['name'])\n",
    "merged_df= merged_df.drop('name', axis = 1)\n",
    "\n",
    "col = merged_df.pop('id') \n",
    "merged_df.insert(0, 'id', col) \n",
    "\n",
    "# Save the final merged DataFrame\n",
    "merged_df.to_csv(f\"{main_path}Twitter/Final/Merged_structures_X.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'json_name', 'row_path', 'col_path_1', 'col_path_2', 'col_path_3',\n",
       "       'col_path_4', 'col_path_5', 'col_path_6', 'col_path_7', 'col_path_8'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_id.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-donations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
