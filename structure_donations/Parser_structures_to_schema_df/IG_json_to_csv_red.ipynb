{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path \n",
    "import re\n",
    "import ast\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/Instagram/Input_test\n"
     ]
    }
   ],
   "source": [
    "main_path = \"/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/\"\n",
    "\n",
    "input_directory = Path(f'{main_path}Instagram/Input')  \n",
    "output_directory = Path(f'{main_path}Instagram/Output')  \n",
    "\n",
    "for file in output_directory.iterdir():\n",
    "   if file.is_file():\n",
    "      file.unlink() \n",
    "\n",
    "max_columns = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_columns):\n",
    "        col_path = [f\"col_path_{i}\" for i in range(1, max_columns)]\n",
    "        col_path_values = [f\"col_path_{i}_values\" for i in range(1, max_columns)]\n",
    "        col_path_list = [f\"col_path_{i}_lIST\" for i in range(1, max_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['col_path_1_values', 'col_path_2_values', 'col_path_3_values', 'col_path_4_values', 'col_path_5_values', 'col_path_6_values', 'col_path_7_values']\n"
     ]
    }
   ],
   "source": [
    "print(col_path_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data(data):\n",
    "    \n",
    "\n",
    "    with open(data, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    print(type(data))\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['col_path_0_values'] = data\n",
    "    df['json_name'] = df.index\n",
    "    df['json_name'] = df['json_name'].str.rsplit(\"/\", n=1).str[-1]\n",
    "    df['file_path'] = df.index\n",
    "    df['file_path'] = df['file_path'].str.replace('^instagram-[^/]+/', '', regex=True)\n",
    "    df['file_path'] = df['file_path'].str.replace(r'message_requests/.*?/message_1\\.json', r'message_requests/username/message_1.json', regex=True)\n",
    "    df['file_path'] = df['file_path'].str.replace(r'(inbox/)[^/]+(?=/message_1\\.json)', r'\\1username', regex=True)\n",
    "   \n",
    "    df['col_path_0_LIST'] = ''\n",
    "\n",
    "\n",
    "\n",
    "    col = df.pop('json_name') \n",
    "    df.insert(0, 'json_name', col)\n",
    "   \n",
    "    col = df.pop('file_path') \n",
    "    df.insert(0, 'file_path', col)\n",
    "    \n",
    "    df.reset_index(inplace = True)\n",
    "    df = df.drop('index', axis = 1)\n",
    "\n",
    "   \n",
    "    \n",
    "    for i in range(1,max_columns):   \n",
    "        df[f'col_path_{i}'] = ''\n",
    "        df[f'col_path_{i}_values'] = ''\n",
    "        df[f'col_path_{i}_LIST'] = ''\n",
    "\n",
    "    index_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['col_path_0_values'] == 'No data':\n",
    "            index_list.append(idx)\n",
    "    \n",
    "    df_no_data = df.loc[index_list]\n",
    "    df = df.drop(index_list)\n",
    "   \n",
    "\n",
    "    \n",
    "    return(df, df_no_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatten_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json(df):\n",
    "    \n",
    "    new_rows = []\n",
    "    current_rows = df.copy()  # snapshot to iterate over\n",
    "\n",
    "    for ix, row in current_rows.iterrows():\n",
    "        value= row['col_path_0_values']\n",
    "            \n",
    "        if isinstance(value, list):\n",
    "                    # Check if list of dicts\n",
    "            if value and isinstance(value[0], dict):\n",
    "                \n",
    "                \n",
    "                seen = set()\n",
    "                unique = []\n",
    "                for d in value:\n",
    "                    # Use json.dumps to get a hashable, sorted representation\n",
    "                    s = json.dumps(d, sort_keys=True)\n",
    "                    if s not in seen:\n",
    "                        seen.add(s)\n",
    "                        unique.append(d)\n",
    "                value = unique\n",
    "                \n",
    "\n",
    "                for item in value:\n",
    "                    new_row = row.copy()\n",
    "                    new_row['col_path_0_values'] = item\n",
    "                    new_row['col_path_0_LIST'] = 'LIST'\n",
    "                    new_rows.append(new_row)\n",
    "\n",
    "        else:\n",
    "            df.at[ix, 'col_path_0_LIST'] = 'NO LIST'\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    \n",
    "    new_rows.clear()\n",
    "\n",
    "           \n",
    "\n",
    "    new_rows = []\n",
    "\n",
    "    for i in range(max_columns):\n",
    "        current_rows = df.copy()  # snapshot to iterate over\n",
    "\n",
    "        for ix, row in current_rows.iterrows():\n",
    "            value = row.get(f'col_path_{i}_values', None)\n",
    "    \n",
    "\n",
    "            if isinstance(value, dict):\n",
    "                for k, v in value.items():\n",
    "                    if isinstance(v, list):\n",
    "                        # Check if list of dicts\n",
    "                        if v and isinstance(v[0], dict):\n",
    "\n",
    "                            seen = set()\n",
    "                            unique = []\n",
    "                            for d in v:\n",
    "                                # Use json.dumps to get a hashable, sorted representation\n",
    "                                s = json.dumps(d, sort_keys=True)\n",
    "                                if s not in seen:\n",
    "                                    seen.add(s)\n",
    "                                    unique.append(d)\n",
    "                            v = unique\n",
    "                            \n",
    "\n",
    "                            for item in v:\n",
    "                                new_row = row.copy()\n",
    "                                new_row[f'col_path_{i+1}_values'] = item\n",
    "                                new_row[f'col_path_{i+1}'] = k\n",
    "                                new_row[f'col_path_{i+1}_LIST'] = 'LIST'\n",
    "                                new_rows.append(new_row)\n",
    "                        else:\n",
    "                            \n",
    "                            v = v[0]\n",
    "                            new_row = row.copy()\n",
    "                            new_row[f'col_path_{i+1}_values'] = v\n",
    "                            new_row[f'col_path_{i+1}'] = k\n",
    "                            new_row[f'col_path_{i+1}_LIST'] = 'LIST'\n",
    "                            new_rows.append(new_row)\n",
    "                    else:\n",
    "                \n",
    "                        new_row = row.copy()\n",
    "                        new_row[f'col_path_{i+1}_values'] = v\n",
    "                        new_row[f'col_path_{i+1}'] = k\n",
    "                        new_row[f'col_path_{i+1}_LIST'] = 'NO LIST'\n",
    "                        new_rows.append(new_row)\n",
    "                \n",
    "                    if isinstance(v, str):\n",
    "                        new_row = row.copy()\n",
    "                        new_row[f'col_path_{i+1}_LIST'] = 'NO LIST'\n",
    "                        new_rows.append(new_row)\n",
    "                \n",
    "                \n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "        new_rows.clear()\n",
    "            \n",
    "                        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df_red = df.drop(columns=[col for col in df.columns if col.endswith(\"_LIST\")])\n",
    "    \n",
    "    df_red['last_valid_index'] = df_red.apply(pd.Series.last_valid_index, axis=1)\n",
    "    df_red = df_red.drop(columns=[col for col in df.columns if col.endswith(\"values\")])\n",
    "\n",
    "    \n",
    "    col_path = [f\"col_path_{i}\" for i in range(1, max_columns)]\n",
    "    col_path = col_path + ['json_name'] + ['file_path'] \n",
    "    df = pd.merge(df, df_red, on = col_path, how='left')\n",
    "    \n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process_col_path()\n",
    "The 'process_col_path()' function checks whether the value in a row of for one of the column paths is actually a datatype and stores this value in a column data_type and replaces the original value with NA. The data types are the lowest level values in the JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types\n",
    "data_types = ['string', 'array', 'number', 'boolean', 'object']\n",
    "\n",
    "\n",
    "\n",
    "# Define the function 'process_col_path()'\n",
    "def process_col_path(df, col_path_values, data_types):\n",
    "    df['data_type'] = ''\n",
    "\n",
    "    \"\"\"\n",
    "    row: Rows in the dataframe\n",
    "    columns: List of column names of column path columns \n",
    "    data_types: List of the values that are data types\n",
    "    \"\"\"\n",
    "    \n",
    "    for ix, row in df.iterrows():\n",
    "        for col in col_path_values:\n",
    "            \n",
    "\n",
    "            #If the value stored in the column is found in the list 'data_types', \n",
    "            if row[col] in data_types:\n",
    "                # this value is placed in the column 'data_type'\n",
    "                value = row[col]\n",
    "                \n",
    "                df.at[ix, 'data_type'] = value\n",
    "            else:\n",
    "                continue\n",
    "               \n",
    "    \n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove_intermediate_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_intermediate_rows(df):\n",
    "    drop_index = []\n",
    "    for ix, row in df.iterrows():\n",
    "            last_value = row[f\"{row['last_valid_index']}\"]\n",
    "           \n",
    "        \n",
    "            if isinstance(last_value, str):\n",
    "                if last_value not in (data_types):\n",
    "\n",
    "                        drop_index.append(ix)\n",
    "            else:\n",
    "                drop_index.append(ix)\n",
    "                  \n",
    "        \n",
    "   \n",
    "    df = df.drop(drop_index)\n",
    "    df = df.drop(columns=[col for col in df.columns if col.endswith(\"values\")])\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path(df, max_columns):\n",
    "    \n",
    "    df['path']= ''\n",
    "    \n",
    "    for ix, row in df.iterrows():\n",
    "        path = []\n",
    "    \n",
    "        for col in [f\"col_path_{i}\" for i in range(1, max_columns)]:\n",
    "            val = row[col]\n",
    "        \n",
    "            if pd.notna(val):\n",
    "\n",
    "                path.append(str(val))\n",
    "\n",
    "        \n",
    "        df.at[ix, 'path'] = path\n",
    "       \n",
    "    \n",
    "        \n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef list_path(df):\\n\\n    df[\\'list_path\\'] = \\'\\'\\n    df[\\'subfield_path\\'] = \\'\\'\\n    df[\\'column_name\\'] = \\'\\'\\n    df[\\'var_type\\'] = \\'\\'\\n\\n\\n\\n    for ix, row in df.iterrows():\\n\\n        path = row[\\'path\\'] \\n\\n        if len(path) > 1:\\n            for i in range(1, len(path)):\\n\\n                if row[f\"col_path_{i}_LIST\"] == \\'LIST\\':\\n                    var_type = \\'list\\'\\n                    list_path = json.dumps(path[:-1])\\n                    subfield_path = path[-1]\\n                    column_name = subfield_path\\n                    subfield_path = json.dumps(subfield_path)\\n                    break\\n\\n                elif row[f\"col_path_{i}_LIST\"] == \"NO LIST\":\\n                    var_type = \\'static\\'\\n                    list_path = np.nan\\n                    subfield_path = json.dumps(path)\\n                    column_name = path[-1]\\n\\n                elif row[f\"col_path_{i}_LIST\"] == \\'nan\\':\\n                    var_type = \\'skip\\'\\n                    list_path = np.nan\\n                    subfield_path = json.dumps(path)\\n                    column_name = path[-1]\\n\\n                else:\\n                    var_type = \\'skip\\'\\n                    list_path = np.nan\\n                    subfield_path = json.dumps(path)\\n                    column_name = path[-1]\\n\\n        else:\\n                if row[\"col_path_0_LIST\"] == \\'LIST\\':\\n                    var_type = \\'list\\'\\n                    list_path = json.dumps(path[:-1])\\n                    subfield_path = path[-1]\\n                    column_name = subfield_path\\n                    subfield_path = json.dumps(subfield_path)\\n                    break\\n\\n                elif row[\"col_path_0_LIST\"] == \"NO LIST\":\\n                    var_type = \\'static\\'\\n                    list_path = np.nan\\n                    subfield_path = json.dumps(path)\\n                    column_name = path[-1]\\n\\n\\n\\n\\n        df.at[ix, \\'list_path\\'] = list_path\\n        df.at[ix, \\'subfield_path\\'] = subfield_path\\n        df.at[ix, \\'column_name\\'] = column_name\\n        df.at[ix, \\'var_type\\'] = var_type\\n\\n\\n\\n\\n    return df\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def list_path(df):\n",
    "    \n",
    "    df['list_path'] = ''\n",
    "    df['subfield_path'] = ''\n",
    "    df['column_name'] = ''\n",
    "    df['var_type'] = ''\n",
    "    \n",
    "\n",
    "\n",
    "    for ix, row in df.iterrows():\n",
    "        \n",
    "        path = row['path'] \n",
    "        \n",
    "        if len(path) > 1:\n",
    "            for i in range(1, len(path)):\n",
    "            \n",
    "                if row[f\"col_path_{i}_LIST\"] == 'LIST':\n",
    "                    var_type = 'list'\n",
    "                    list_path = json.dumps(path[:-1])\n",
    "                    subfield_path = path[-1]\n",
    "                    column_name = subfield_path\n",
    "                    subfield_path = json.dumps(subfield_path)\n",
    "                    break\n",
    "\n",
    "                elif row[f\"col_path_{i}_LIST\"] == \"NO LIST\":\n",
    "                    var_type = 'static'\n",
    "                    list_path = np.nan\n",
    "                    subfield_path = json.dumps(path)\n",
    "                    column_name = path[-1]\n",
    "                \n",
    "                elif row[f\"col_path_{i}_LIST\"] == 'nan':\n",
    "                    var_type = 'skip'\n",
    "                    list_path = np.nan\n",
    "                    subfield_path = json.dumps(path)\n",
    "                    column_name = path[-1]\n",
    "                \n",
    "                else:\n",
    "                    var_type = 'skip'\n",
    "                    list_path = np.nan\n",
    "                    subfield_path = json.dumps(path)\n",
    "                    column_name = path[-1]\n",
    "\n",
    "        else:\n",
    "                if row[\"col_path_0_LIST\"] == 'LIST':\n",
    "                    var_type = 'list'\n",
    "                    list_path = json.dumps(path[:-1])\n",
    "                    subfield_path = path[-1]\n",
    "                    column_name = subfield_path\n",
    "                    subfield_path = json.dumps(subfield_path)\n",
    "                    break\n",
    "\n",
    "                elif row[\"col_path_0_LIST\"] == \"NO LIST\":\n",
    "                    var_type = 'static'\n",
    "                    list_path = np.nan\n",
    "                    subfield_path = json.dumps(path)\n",
    "                    column_name = path[-1]\n",
    "\n",
    "\n",
    "           \n",
    "    \n",
    "        df.at[ix, 'list_path'] = list_path\n",
    "        df.at[ix, 'subfield_path'] = subfield_path\n",
    "        df.at[ix, 'column_name'] = column_name\n",
    "        df.at[ix, 'var_type'] = var_type\n",
    "\n",
    "    \n",
    "      \n",
    "        \n",
    "    return df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_path(df):\n",
    "    \n",
    "    df['list_path'] = ''\n",
    "    df['subfield_path'] = ''\n",
    "    df['column_name'] = ''\n",
    "    df['var_type'] = ''\n",
    "    \n",
    "    new_rows = []\n",
    "    delete_rows = []\n",
    "\n",
    "    for ix, row in df.iterrows():\n",
    "        \n",
    "        path = row['path'] \n",
    "        \n",
    "        \n",
    "        for i in reversed(range(len(path))):\n",
    "           \n",
    "            if row[f\"col_path_{i}_LIST\"] == 'LIST':\n",
    "                \n",
    "                new_row = row.copy()\n",
    "                delete_rows.append(ix)\n",
    "                new_row['var_type'] = 'list'\n",
    "                new_row['list_path'] = path[:i]\n",
    "                new_row['subfield_path'] = path[i:]\n",
    "                new_row['column_name'] = path[-1]\n",
    "                new_rows.append(new_row)\n",
    "                break\n",
    "\n",
    "            elif row[f\"col_path_{i}_LIST\"] == \"NO LIST\":\n",
    "                var_type = 'static'\n",
    "                subfield_path = path\n",
    "                column_name = path[-1]\n",
    "                \n",
    "\n",
    "            elif row[f\"col_path_{i}_LIST\"] == 'nan':\n",
    "                var_type = 'skip'\n",
    "                subfield_path = path\n",
    "                column_name = path[-1]\n",
    "               \n",
    "            else:\n",
    "                var_type = 'skip'\n",
    "                subfield_path = path\n",
    "                column_name = path[-1]\n",
    "\n",
    "\n",
    "           \n",
    "        df.at[ix, 'subfield_path'] = subfield_path\n",
    "        df.at[ix, 'column_name'] = column_name\n",
    "        df.at[ix, 'var_type'] = var_type\n",
    "\n",
    "    df = df.drop(index=delete_rows).reset_index(drop=True)\n",
    "    df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    new_rows.clear()\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "      \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_and_store()\n",
    "This function orders the columns in the DataFrame, resets the index, fills the NA and saves the DataFrame as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_store(df, df_no_data, file_name):\n",
    "\n",
    "    \"\"\"\n",
    "    df: The dataframe that will be cleaned and stored\n",
    "    file_name: The filename of data structure that is being processed\n",
    "    \"\"\"\n",
    "\n",
    "  \n",
    "   \n",
    "    #df=  df[df['col_path_1'] != 'Direct Message']\n",
    "    \n",
    "    df = pd.concat([df, df_no_data], ignore_index=True)\n",
    "    \n",
    "  \n",
    "    df = df[['json_name','column_name', 'path', 'list_path', 'subfield_path', 'var_type', 'data_type', 'file_path']]\n",
    "    df = df.astype(str)\n",
    "    df = df.drop_duplicates()\n",
    "    # Save the DataFrame \n",
    "    df.to_csv(f\"{main_path}Instagram/Output/Output_\" + file_name + '.csv', index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### structure_donations()\n",
    "The structure_donations() function executes all functions above and results in a saved DataFrame for each data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "def structure_donations(data, col_path, max_columns):\n",
    "     # Store the path to the data structure\n",
    "    data = Path(data)  \n",
    "    \n",
    "    # Save teh file name of the data structure\n",
    "    file_name = Path(data).stem \n",
    "\n",
    "    df, df_no_data = loading_data(data)\n",
    "    print('FINISH: loading_data()')\n",
    "    df = flatten_json(df)\n",
    "    print('FINISH: flatten_json()')\n",
    "    df = process_col_path(df, col_path_values, data_types)\n",
    "    print('FINISH: process_col_path()')\n",
    "    df = remove_intermediate_rows(df)\n",
    "    print('FINISH: remove_intermediate_rows()')\n",
    "    df = extract_path(df, max_columns)\n",
    "    print('FINISH: extract_path()')\n",
    "    df = list_path(df)\n",
    "    print('FINISH: list_path()')\n",
    "    df = clean_and_store(df, df_no_data, file_name)\n",
    "    print('FINISH: clean_and_store()')\n",
    "\n",
    "    del df,df_no_data, data\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute 'structure_donations()': Transform data structures from JSON format to tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE NAME: 08-04-2025-json_structure instagram.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: flatten_json()\n",
      "FINISH: process_col_path()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88159/546882260.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "FILE NAME: IG_structure_instagram-x.nienke_-2025-02-18-sCPTrHuB.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88159/546882260.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: flatten_json()\n",
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "FILE NAME: IG_structure_instagram_takeout_ro.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n",
      "FINISH: flatten_json()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88159/546882260.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "FILE NAME: IG_json_structure_CASAS.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88159/546882260.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: flatten_json()\n",
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "FILE NAME: IG_structure_instagram-geodag91-2024-11-20-poQTe0ag.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88159/546882260.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: flatten_json()\n",
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "FILE NAME: json_structure_ig.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88159/546882260.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: flatten_json()\n",
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "FILE NAME: IG_structure_instagram_takeout_es.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88159/546882260.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: flatten_json()\n",
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "FILE NAME: json_structure-instagram-mfb.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88159/546882260.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: flatten_json()\n",
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "FILE NAME: json_structure_1_RB_Instagram.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88159/546882260.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: flatten_json()\n",
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "FILE NAME: instagram-geodag91-2024-11-20-poQTe0ag-20250214T091036Z-001_structure.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n",
      "FINISH: flatten_json()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88159/546882260.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "FILE NAME: IG_structure_instagram-geodag91-2024-11-20-poQTe0ag-20250214T091036Z-001.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88159/546882260.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: flatten_json()\n",
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "FILE NAME: nv_insta.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88159/546882260.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: flatten_json()\n",
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "FILE NAME: IG_structure_instagram_takeout_dutch.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88159/546882260.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: flatten_json()\n",
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n"
     ]
    }
   ],
   "source": [
    "# Execute the 'structure_donations()' function for each file (data structure) in the input directory\n",
    "for file in input_directory.iterdir():  \n",
    "    if file.is_file():  \n",
    "        print(\"FILE NAME:\", file.name)\n",
    "        result = structure_donations(file, col_path, max_columns)\n",
    "        del result\n",
    "        gc.collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all data structures into one schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing CSV files\n",
    "output_path = f\"{main_path}Instagram/Output\"\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = list(Path(output_path).glob(\"*.csv\"))\n",
    "\n",
    "# Load all CSVs into a list of DataFrames\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# Drop rows that are completely identical across all columns\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Id creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df['name'] = merged_df['json_name'].str.replace(\".json\", \"\")\n",
    "\n",
    "id_df = merged_df[['name', 'path', 'file_path']]\n",
    "id_df = id_df.drop_duplicates()\n",
    "id_df['id'] = ''\n",
    "\n",
    "id_df['folder1'] = id_df['file_path'].str.strip('/').str.split('/').str[-2]\n",
    "id_df['folder2'] = id_df['file_path'].str.strip('/').str.split('/').str[-3]\n",
    "\n",
    "\n",
    "for n in range(1,4):\n",
    "    duplicates = id_df[id_df.duplicated(subset='id', keep=False)]\n",
    "\n",
    "    for i, row in duplicates.iterrows():\n",
    "        path = row['path']\n",
    "        name = row['name']  \n",
    "        folder1 = row['folder1']\n",
    "\n",
    "        if pd.notna(path):\n",
    "            \n",
    "            path =  ast.literal_eval(path)\n",
    "            \n",
    "            path_rest = path[-n:]\n",
    "        \n",
    "\n",
    "            if not isinstance(path_rest, list):\n",
    "                path_rest = [path_rest]\n",
    "\n",
    "\n",
    "            id_list = [folder1] + [name] + path_rest\n",
    "\n",
    "            new_id = ':'.join(id_list)\n",
    "\n",
    "        else:\n",
    "\n",
    "            new_id = name\n",
    "\n",
    "        id_df.at[i, 'id'] = new_id  \n",
    "\n",
    "\n",
    "\n",
    "duplicates = id_df[id_df.duplicated(subset='id', keep=False)]\n",
    "\n",
    "for i, row in duplicates.iterrows():\n",
    "    folder2 = row['folder2']\n",
    "    id = row['id']\n",
    "    new_id =  f'{folder2}:{id}'\n",
    "    id_df.at[i, 'id'] = new_id  \n",
    "\n",
    "\n",
    "\n",
    "merged_df = pd.merge(merged_df, id_df, on = ['name', 'path','file_path'], how = 'left')\n",
    "\n",
    "col = merged_df.pop('id') \n",
    "merged_df.insert(0, 'id', col) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID duplicate flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88159/2402599662.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dup['duplicate_flag'] = 'Yes'\n"
     ]
    }
   ],
   "source": [
    "merged_df = merged_df.astype(str)\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "dup = merged_df[merged_df.duplicated('id', keep= False)]\n",
    "dup['duplicate_flag'] = 'Yes'\n",
    "dup = dup[['id', 'duplicate_flag']]\n",
    "merged_df = pd.merge(merged_df, dup, on = 'id', how = 'left')\n",
    "merged_df['duplicate_flag'] = merged_df['duplicate_flag'].fillna(value = 'No')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "keep_columns = ['json_name','id', 'column_name', \n",
    "                 'path', 'list_path', 'subfield_path', \n",
    "                 'var_type', 'data_type', 'file_path', 'duplicate_flag']\n",
    "\n",
    "\n",
    "merged_df = merged_df[keep_columns ]\n",
    "\n",
    "merged_df = merged_df.astype(str)\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# Save the final merged DataFrame\n",
    "merged_df.to_csv(f\"{main_path}Instagram/Final/IG_Merged_structures.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-donations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
