{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Instagram Data Structures into a Schema_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The purpose of his jupyter notebook is to parse the collected data structures in earlier iterations of utilising the data donation tool into a schema_df which can be used to inform future iterations of the data donation tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path  \n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns_creator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_col_path = 6\n",
    "def columns_creator(max_col_path):\n",
    "    col_path = [f\"col_path_{i}\" for i in range(1, max_col_path+1)]\n",
    "    col_var = ['value']+[f\"col_path_{i}_values\" for i in range(1, max_col_path)]\n",
    "    get_var = [f\"col_path_{i}\" for i in range(1, max_col_path+1)]\n",
    "    unlist_var = [f\"col_path_{i}_values\" for i in range(1, max_col_path+1)]\n",
    "    list_var = [f\"col_path_{i}\" for i in range(2, max_col_path+1)]\n",
    "    col_path_list = [f\"col_path_{i}_LIST\" for i in range(2, max_col_path+1)]\n",
    "\n",
    "    \n",
    "    return col_path, col_var, get_var, unlist_var, list_var, col_path_list\n",
    "\n",
    "col_path, col_var, get_var, unlist_var,list_var, col_path_list = columns_creator(max_col_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process_col_path()\n",
    "The 'process_col_path()' function checks whether the value in a row of for one of the column paths is actually a datatype and stores this value in a column data_type and replaces the original value with NA. The data types are the lowest level values in the JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types\n",
    "data_types = ['string', 'array', 'number', 'boolean', 'object', 'str', 'int', 'float', 'bool', 'dict', 'list']\n",
    "\n",
    "#Define the column names\n",
    "columns = col_path\n",
    "\n",
    "\n",
    "\n",
    "# Define the function 'process_col_path()'\n",
    "def process_col_path(row, columns, data_types):\n",
    "\n",
    "    \"\"\"\n",
    "    row: Rows in the dataframe\n",
    "    columns: List of column names of column path columns \n",
    "    data_types: List of the values that are data types\n",
    "    \"\"\"\n",
    "\n",
    "    row['data_type'] = ''\n",
    "    for column in columns:\n",
    "\n",
    "        #If the value stored in the column is found in the list 'data_types', \n",
    "        if row[column] in data_types:\n",
    "            # this value is placed in the column 'data_type'\n",
    "            row['data_type'] = row[column]\n",
    "            # and the original value is replaced with NA\n",
    "            row[column] = np.nan\n",
    "\n",
    "        #If the value is not found in the 'data_types' list, the original value is returned\n",
    "        else:\n",
    "            row[column]\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file_paths()\n",
    "The 'file_paths()' function splits up the paths to where the JSON file is stored in the folder and provides the name of the json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_paths(df):\n",
    "    \n",
    "    df['value'] = df['value'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### string_to_dict()\n",
    "As the JSON files are loaded as strings, they need to be converted to dictionaries to extract the values and be cleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_dict(s):\n",
    "    # Check if the items needed for splitting are present. If not present it does not need to be splitted and the orginal value is returned\n",
    "    if ',' not in s and ':' not in s:\n",
    "        return s\n",
    "    \n",
    "    # Create an empty dictionary\n",
    "    result = {}\n",
    "\n",
    "    # Split the items by comma (split into key-value pairs)\n",
    "    items = s.split(',')\n",
    "\n",
    "    # For each item in the original dictionary\n",
    "    for item in items:\n",
    "        # Check if it contains a key-value pair, if not continue\n",
    "        if ':' not in item:\n",
    "            continue  \n",
    "\n",
    "        # Split the key-value pair into a variable 'key' and a variable 'value'\n",
    "        key, value = item.split(':', 1)  # use maxsplit=1 to avoid unpacking issue\n",
    "\n",
    "        # Try to strip any white spaces from the keys and values \n",
    "        try:\n",
    "            key = eval(key.strip())\n",
    "            value = eval(value.strip())\n",
    "        # If not possible, continue\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        # Save the converted and cleaned dictionary\n",
    "        result[key] = value\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detect_list()\n",
    "\n",
    "Due to lists in unexpected places in the JSON files and lengthy json paths, we need to identify the positions of lists to later select the correct get() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign list if the data in the original structure is a list\n",
    "    \n",
    "def detect_list(x):\n",
    "    # If the data type of the value is list, 'LIST' is assigned in the '_LIST' columns (see'column_paths())\n",
    "    if isinstance(x, list):\n",
    "        return 'LIST'\n",
    "    # If the value is missing, 'MISSING' is assigned\n",
    "    elif pd.isna(x):\n",
    "        return 'MISSING'\n",
    "    # If the value is 'No data' (there is an empty place holder) 'MISSING' is assigned\n",
    "    elif x == 'No data':\n",
    "            return 'MISSING'\n",
    "    # if the value is not missing and is not a list, 'NO LIST' is assigned\n",
    "    else:\n",
    "        return 'NO LIST'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns_paths()\n",
    "Through this function, we map the json paths to each value by putting the keys in separate columns: colpath_{1,2,3,4,5} (see 'row_column_paths()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_paths(df, col_var, get_var, unlist_var, list_var):\n",
    "\n",
    "    # Convert to dict if it's a string\n",
    "    df[col_var] = df[col_var].apply(\n",
    "        lambda x: string_to_dict(x) if isinstance(x, str) else x)\n",
    "    \n",
    "        # Extract nested values using keys\n",
    "    \"\"\"\n",
    "    1. Selects the key obtained in the previous iteration stored in 'get_var'\n",
    "    2. The key put into a get() function which extracts values of a dictionary based on the keys\n",
    "    3. The get() function extracts the values from the (nested) dictionary stored in 'col_var'\n",
    "    4. If the get() function fails, None is returned\n",
    "    5. The steps above are only performed if the value stored in 'col_var' is a dictionary, otherwise None is returned\n",
    "    6. All steps above are executed for each row in the df\n",
    "    \"\"\"\n",
    "\n",
    "    df[get_var] = df.apply(\n",
    "    lambda row: list(row[col_var].keys()) if isinstance(row[col_var], dict) else row[col_var], axis=1)\n",
    "\n",
    "    \n",
    "    df = df.explode(get_var)\n",
    "\n",
    "    \n",
    "    # Extract nested values using keys\n",
    "    df[unlist_var] = df.apply(lambda row: row[col_var].get(row[get_var], None) if isinstance (row[col_var], dict) else None, axis=1)\n",
    "\n",
    "    # For each each column_path, check if the value contains a list\n",
    "    df[f'{list_var}_LIST'] = df[unlist_var].apply(detect_list)\n",
    "    \n",
    "\n",
    "    df[unlist_var] = df[unlist_var].apply(\n",
    "        lambda x: [x] if not isinstance(x, list) else x)\n",
    "\n",
    "    df = df.explode(unlist_var)\n",
    "    \n",
    "     # Unlist data to avoid double nested lists \n",
    "    df[unlist_var] = df[unlist_var].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['col_path_1_values', 'col_path_2_values', 'col_path_3_values', 'col_path_4_values', 'col_path_5_values', 'col_path_6_values']\n",
      "['col_path_2', 'col_path_3', 'col_path_4', 'col_path_5', 'col_path_6']\n",
      "['value', 'col_path_1_values', 'col_path_2_values', 'col_path_3_values', 'col_path_4_values', 'col_path_5_values']\n",
      "['col_path_1', 'col_path_2', 'col_path_3', 'col_path_4', 'col_path_5', 'col_path_6']\n"
     ]
    }
   ],
   "source": [
    "print(unlist_var)\n",
    "print(list_var)\n",
    "print(col_var)\n",
    "print(col_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### row_column_paths()\n",
    "This function prepares the row paths and consequently executes the column_paths function to obtain the paths of keys (stored in individual columns) to reach the lowest value which is where the actual data is stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_column_paths(df, col_path, col_var, get_var, unlist_var, list_var):\n",
    "    # Initialize the new columns\n",
    "    df['row_path'] = df['variable'] \n",
    "    df[col_path] = np.nan\n",
    "\n",
    "    # Initiate the list of columns needed to inform the function column_paths()\n",
    "    col_var = col_var\n",
    "    get_var = get_var \n",
    "    unlist_var = unlist_var\n",
    "    list_var =  list_var\n",
    "\n",
    "    # Execute the colums_path() function\n",
    "    # zip is necessary due to the large number of variables needed in the column_paths function\n",
    "    for r, g, u, l in zip(col_var, get_var, unlist_var, list_var):\n",
    "        df = column_paths(df, r, g, u, l)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_and_store()\n",
    "This function orders the columns in the DataFrame, resets the index, fills the NA and saves the DataFrame as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_store(df, file_name):\n",
    "\n",
    "    \"\"\"\n",
    "    df: The dataframe that will be cleaned and stored\n",
    "    file_name: The filename of data structure that is being processed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reorder the columns in the df\n",
    "    df = df[['variable', 'value', 'row_path', \n",
    "                    'col_path_1',  \n",
    "                     'col_path_2', 'col_path_2_LIST',\n",
    "                     'col_path_3',  'col_path_3_LIST',\n",
    "                     'col_path_4', 'col_path_4_LIST',\n",
    "                     'col_path_5', 'col_path_5_LIST',\n",
    "                     'col_path_6', 'col_path_6_LIST',\n",
    "                     'data_type']]\n",
    "    \n",
    "    \n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # fill na values with 'Missing'\n",
    "    df = df.fillna('Missing')\n",
    "\n",
    "\n",
    "    # Save the DataFrame \n",
    "    df.to_csv(f\"{main_path}TikTok/Output/Output_\" + file_name + '.csv', index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path(df, max_col_path) -> tuple[str, ...]:\n",
    "    path = []\n",
    "    for col in [\"row_path\"] + [f\"col_path_{i}\" for i in range(1, max_col_path)]:\n",
    "        val = df[col]\n",
    "        if pd.notna(val) and str(val) != \"Missing\":\n",
    "            path.append(str(val).strip())\n",
    "    return tuple(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### structure_donations()\n",
    "The structure_donations() function executes all functions above and results in a saved DataFrame for each data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_donations(data):\n",
    "\n",
    "    \"\"\"\n",
    "    data: The unprocessed data structure JSON that will be processed\n",
    "    \"\"\"\n",
    "\n",
    "    # Store the path to the data structure\n",
    "    data = Path(data)  \n",
    "    \n",
    "    # Save teh file name of the data structure\n",
    "    file_name = Path(data).stem \n",
    "\n",
    "    # Load JSON file (data structures)\n",
    "    with open(data, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Strip top-level key\n",
    "    data = list(data.values())[0]\n",
    "\n",
    "\n",
    "    # Flatten JSON (handling nested structures)\n",
    "    df = pd.json_normalize(data, max_level=0)\n",
    "\n",
    "    # Extract column names\n",
    "    cols = df.columns.tolist()\n",
    "\n",
    "\n",
    "    # From wide to long df\n",
    "    df = pd.melt(df, value_vars= cols)\n",
    "\n",
    "    # Execute the 'file_paths()' function and store the result in df\n",
    "    df = file_paths(df)\n",
    "\n",
    "    # Execute the 'row_column_paths()' function and store the result in df\n",
    "    df = row_column_paths(df, col_path, col_var, get_var, unlist_var, list_var)\n",
    "    # Execute the 'process_col_path()' function and store the result in df\n",
    "    df = df.apply(lambda row: process_col_path(row, columns, data_types), axis=1)\n",
    "\n",
    "    # Execute the 'clean_and_store()' function and store the result in df\n",
    "    df = clean_and_store(df, file_name)\n",
    "    \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute 'structure_donations()': Transform data structures from JSON format to tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/TikTok/Input_test\n"
     ]
    }
   ],
   "source": [
    "input_directory = Path(f'{main_path}TikTok/Input_test')  \n",
    "print(input_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the 'structure_donations()' function for each file (data structure) in the input directory\n",
    "for file in input_directory.iterdir():  \n",
    "    if file.is_file():  \n",
    "        structure_donations(file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all data structures into one schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_686685/4250677247.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged_df = merged_df.replace('Missing', np.nan)\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing CSV files\n",
    "output_path = f\"{main_path}TikTok/Output\"\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = list(Path(output_path).glob(\"*.csv\"))\n",
    "\n",
    "# Load all CSVs into a list of DataFrames\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "col_subset = merged_df.columns.tolist()\n",
    "col_subset.remove('value')\n",
    "\n",
    "# Drop rows that are completely identical across all columns except value\n",
    "merged_df = merged_df.drop_duplicates(subset= col_subset)\n",
    "\n",
    "\n",
    "df_index = merged_df[(merged_df['row_path'] == 'Direct Message')].index\n",
    "merged_df.drop(df_index, inplace= True)\n",
    "\n",
    "# Replace 'Missing' with NaN\n",
    "merged_df = merged_df.replace('Missing', np.nan)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_subset = merged_df.columns.tolist()\n",
    "col_subset.remove('value')\n",
    "\n",
    "# Drop rows that are completely identical across all columns\n",
    "merged_df = merged_df.drop_duplicates(subset= col_subset)\n",
    "\n",
    "merged_df['path'] = merged_df.apply(lambda x: extract_path(x, max_col_path), axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up and reduce number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_path(row):\n",
    "    path = row['path'] \n",
    "    for i in range(len(path)):\n",
    "        if row[f\"col_path_{i+2}_LIST\"] == \"LIST\":\n",
    "            var_type = 'list'\n",
    "            list_path = json.dumps(path[:i+2])\n",
    "            subfield_path = path[-1]\n",
    "            column_name = subfield_path\n",
    "            subfield_path = json.dumps(subfield_path)\n",
    "        elif row[f\"col_path_{i+2}_LIST\"] == \"NO LIST\":\n",
    "            var_type = 'static'\n",
    "            list_path = np.nan\n",
    "            subfield_path = json.dumps(path)\n",
    "            column_name = path[-1]\n",
    "        elif row[f\"col_path_{i+2}_LIST\"] == \"MISSING\" :\n",
    "            var_type = 'skip'\n",
    "            list_path = np.nan\n",
    "            subfield_path = json.dumps(path)\n",
    "            column_name = path[-1]\n",
    "        return list_path, subfield_path, column_name, var_type\n",
    "\n",
    "\n",
    "merged_df[['list_path', 'subfield_path', 'column_name', 'var_type']] = merged_df.apply(lambda x: pd.Series(list_path(x)), axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Id creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = merged_df[['variable', 'path']]\n",
    "id_df = id_df.drop_duplicates()\n",
    "id_df['id'] = ''\n",
    "\n",
    "for n in range(1,max_col_path):\n",
    "    duplicates = id_df[id_df.duplicated(subset='id', keep=False)]\n",
    "\n",
    "    for i, row in duplicates.iterrows():\n",
    "        path = list(row['path'])\n",
    "        path_zero = path[0]\n",
    "        path_rest = path[-n:]  # Varies each loop to attempt uniqueness\n",
    "\n",
    "        id_list = [path_zero] + path_rest\n",
    "        new_id = ':'.join(id_list)\n",
    "\n",
    "        id_df.at[i, 'id'] = new_id  # Properly update the DataFrame\n",
    "\n",
    "merged_df = pd.merge(merged_df, id_df, on = ['variable', 'path'], how = 'left')\n",
    "\n",
    "col = merged_df.pop('id') \n",
    "merged_df.insert(0, 'id', col) \n",
    "#merged_df['id']= merged_df['id'].fillna(merged_df['variable'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_686685/3696797969.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dup['duplicate_flag'] = 'Yes'\n"
     ]
    }
   ],
   "source": [
    "merged_df = merged_df.drop_duplicates()\n",
    "dup = merged_df[merged_df.duplicated('id', keep= False)]\n",
    "dup['duplicate_flag'] = 'Yes'\n",
    "dup = dup[['id', 'duplicate_flag']]\n",
    "merged_df = pd.merge(merged_df, dup, on = 'id', how = 'left')\n",
    "merged_df['duplicate_flag'] = merged_df['duplicate_flag'].fillna(value = 'No')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'column_name', 'variable', 'path', 'list_path', 'subfield_path', 'var_type', 'data_type', 'row_path', 'col_path_1', 'col_path_2', 'col_path_3', 'col_path_4', 'col_path_5', 'col_path_6', 'json_name', 'duplicate_flag']\n"
     ]
    }
   ],
   "source": [
    "merged_df['json_name'] = ''\n",
    "\n",
    "keep_columns = ['id', 'column_name',  'variable',\n",
    "                 'path', 'list_path', 'subfield_path', \n",
    "                 'var_type', 'data_type','row_path'] + col_path + ['json_name', 'duplicate_flag']\n",
    "\n",
    "print(keep_columns)\n",
    "\n",
    "merged_df = merged_df[keep_columns ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final merged DataFrame\n",
    "merged_df.to_csv(f\"{main_path}TikTok/Final/Merged_structures_TT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 17)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-donations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
