{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path \n",
    "import re\n",
    "import ast\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/Twitter/Input_test\n"
     ]
    }
   ],
   "source": [
    "main_path = \"/home/rvissche/Nextcloud/What-If/what-if-data-donation/what-if-data-donation/structure_donations/Processed_structure_donations/\"\n",
    "\n",
    "input_directory = Path(f'{main_path}Twitter/Input_test')  \n",
    "print(input_directory)\n",
    "max_columns = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_columns):\n",
    "        col_path = [f\"col_path_{i}\" for i in range(1, max_columns)]\n",
    "        col_path_values = [f\"col_path_{i}_values\" for i in range(1, max_columns)]\n",
    "        col_path_list = [f\"col_path_{i}_lIST\" for i in range(1, max_columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data(data):\n",
    "    \n",
    "\n",
    "    with open(data, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    print(type(data))\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['col_path_0_values'] = data\n",
    "    df['json_name'] = df.index\n",
    "    df['json_name'] = df['json_name'].str.replace(r'^data/', '', regex=True)\n",
    "    df['col_path_0_LIST'] = ''\n",
    "\n",
    "   \n",
    "\n",
    "    col = df.pop('json_name') \n",
    "    df.insert(0, 'json_name', col)\n",
    "    \n",
    "    df.reset_index(inplace = True)\n",
    "    df = df.drop('index', axis = 1)\n",
    "\n",
    "   \n",
    "    \n",
    "    for i in range(1,max_columns):   \n",
    "        df[f'col_path_{i}'] = ''\n",
    "        df[f'col_path_{i}_values'] = ''\n",
    "        df[f'col_path_{i}_LIST'] = ''\n",
    "\n",
    "    index_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['col_path_0_values'] == 'No data':\n",
    "            index_list.append(idx)\n",
    "    \n",
    "    df_no_data = df.loc[index_list]\n",
    "    df = df.drop(index_list)\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    return(df, df_no_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatten_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json(df):\n",
    "    \n",
    "    new_rows = []\n",
    "    current_rows = df.copy()  # snapshot to iterate over\n",
    "\n",
    "    for ix, row in current_rows.iterrows():\n",
    "        value= row['col_path_0_values']\n",
    "            \n",
    "        if isinstance(value, list):\n",
    "                    # Check if list of dicts\n",
    "            if value and isinstance(value[0], dict):\n",
    "                list_holder = 'LIST'\n",
    "                \n",
    "                seen = set()\n",
    "                unique = []\n",
    "                for d in value:\n",
    "                    # Use json.dumps to get a hashable, sorted representation\n",
    "                    s = json.dumps(d, sort_keys=True)\n",
    "                    if s not in seen:\n",
    "                        seen.add(s)\n",
    "                        unique.append(d)\n",
    "                value = unique\n",
    "                \n",
    "\n",
    "                for item in value:\n",
    "                    new_row = row.copy()\n",
    "                    new_row['col_path_0_values'] = item\n",
    "                    new_row['col_path_0_LIST'] = list_holder\n",
    "                    new_rows.append(new_row)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    \n",
    "    new_rows.clear()\n",
    "\n",
    "           \n",
    "\n",
    "    new_rows = []\n",
    "\n",
    "    for i in range(max_columns):\n",
    "        current_rows = df.copy()  # snapshot to iterate over\n",
    "\n",
    "        for ix, row in current_rows.iterrows():\n",
    "            value = row.get(f'col_path_{i}_values', None)\n",
    "    \n",
    "\n",
    "            if isinstance(value, dict):\n",
    "                for k, v in value.items():\n",
    "                    if isinstance(v, list):\n",
    "                        # Check if list of dicts\n",
    "                        if v and isinstance(v[0], dict):\n",
    "\n",
    "                            seen = set()\n",
    "                            unique = []\n",
    "                            for d in v:\n",
    "                                # Use json.dumps to get a hashable, sorted representation\n",
    "                                s = json.dumps(d, sort_keys=True)\n",
    "                                if s not in seen:\n",
    "                                    seen.add(s)\n",
    "                                    unique.append(d)\n",
    "                            v = unique\n",
    "                            \n",
    "\n",
    "                            for item in v:\n",
    "                                new_row = row.copy()\n",
    "                                new_row[f'col_path_{i+1}_values'] = item\n",
    "                                new_row[f'col_path_{i+1}'] = k\n",
    "                                new_row[f'col_path_{i+1}_LIST'] = 'LIST'\n",
    "                                new_rows.append(new_row)\n",
    "                        else:\n",
    "                            # Regular list (e.g., strings, ints)\n",
    "                            v = v[0]\n",
    "                            new_row = row.copy()\n",
    "                            new_row[f'col_path_{i+1}_values'] = v\n",
    "                            new_row[f'col_path_{i+1}'] = k\n",
    "                            new_row[f'col_path_{i+1}_LIST'] = 'LIST'\n",
    "                            new_rows.append(new_row)\n",
    "                    else:\n",
    "                \n",
    "                        new_row = row.copy()\n",
    "                        new_row[f'col_path_{i+1}_values'] = v\n",
    "                        new_row[f'col_path_{i+1}'] = k\n",
    "                        new_row[f'col_path_{i+1}_LIST'] = 'NO LIST'\n",
    "                        new_rows.append(new_row)\n",
    "                \n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "        new_rows.clear()\n",
    "            \n",
    "                        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df_red = df.drop(columns=[col for col in df.columns if col.endswith(\"_LIST\")])\n",
    "    \n",
    "    df_red['last_valid_index'] = df_red.apply(pd.Series.last_valid_index, axis=1)\n",
    "    df_red = df_red.drop(columns=[col for col in df.columns if col.endswith(\"values\")])\n",
    "\n",
    "    \n",
    "    col_path = [f\"col_path_{i}\" for i in range(1, max_columns)]\n",
    "    col_path = col_path + ['json_name']\n",
    "    df = pd.merge(df, df_red, on = col_path, how='left')\n",
    "    \n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process_col_path()\n",
    "The 'process_col_path()' function checks whether the value in a row of for one of the column paths is actually a datatype and stores this value in a column data_type and replaces the original value with NA. The data types are the lowest level values in the JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types\n",
    "data_types = ['string', 'array', 'number', 'boolean', 'object']\n",
    "\n",
    "\n",
    "\n",
    "# Define the function 'process_col_path()'\n",
    "def process_col_path(df, col_path_values, data_types):\n",
    "    df['data_type'] = ''\n",
    "\n",
    "    \"\"\"\n",
    "    row: Rows in the dataframe\n",
    "    columns: List of column names of column path columns \n",
    "    data_types: List of the values that are data types\n",
    "    \"\"\"\n",
    "    \n",
    "    for ix, row in df.iterrows():\n",
    "        for col in col_path_values:\n",
    "            \n",
    "\n",
    "            #If the value stored in the column is found in the list 'data_types', \n",
    "            if row[col] in data_types:\n",
    "                # this value is placed in the column 'data_type'\n",
    "                value = row[col]\n",
    "                \n",
    "                df.at[ix, 'data_type'] = value\n",
    "            else:\n",
    "                continue\n",
    "               \n",
    "    \n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove_intermediate_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_intermediate_rows(df):\n",
    "    drop_index = []\n",
    "    for ix, row in df.iterrows():\n",
    "            last_value = row[f\"{row['last_valid_index']}\"]\n",
    "           \n",
    "        \n",
    "            if isinstance(last_value, str):\n",
    "                if last_value in (data_types):\n",
    "                        col =  row['last_valid_index']\n",
    "                        col_level = re.findall(r'\\d+', col)\n",
    "                        col_level = col_level[0]\n",
    "                                \n",
    "                                \n",
    "                        df.at[ix, f\"col_path_{col_level}_LIST\"] = np.nan\n",
    "\n",
    "                                \n",
    "                else:\n",
    "                        drop_index.append(ix)\n",
    "            else:\n",
    "                drop_index.append(ix)\n",
    "                  \n",
    "        \n",
    "   \n",
    "    df = df.drop(drop_index)\n",
    "    df = df.drop(columns=[col for col in df.columns if col.endswith(\"values\")])\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path(df, max_columns):\n",
    "    \n",
    "    df['path']= ''\n",
    "    \n",
    "    for ix, row in df.iterrows():\n",
    "        path = []\n",
    "    \n",
    "        for col in [f\"col_path_{i}\" for i in range(1, max_columns)]:\n",
    "            val = row[col]\n",
    "        \n",
    "            if pd.notna(val):\n",
    "\n",
    "                path.append(str(val))\n",
    "\n",
    "        \n",
    "        df.at[ix, 'path'] = path\n",
    "       \n",
    "    \n",
    "        \n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_path(df):\n",
    "    \n",
    "    df['list_path'] = ''\n",
    "    df['subfield_path'] = ''\n",
    "    df['column_name'] = ''\n",
    "    df['var_type'] = ''\n",
    "    \n",
    "    new_rows = []\n",
    "    delete_rows = []\n",
    "\n",
    "    for ix, row in df.iterrows():\n",
    "        \n",
    "        path = row['path'] \n",
    "        \n",
    "        \n",
    "        for i in reversed(range(len(path))):\n",
    "           \n",
    "            if row[f\"col_path_{i}_LIST\"] == 'LIST':\n",
    "                \n",
    "                new_row = row.copy()\n",
    "                delete_rows.append(row)\n",
    "                new_row['var_type'] = 'list'\n",
    "                new_row['list_path'] = path[:i]\n",
    "                new_row['subfield_path'] = path[i:]\n",
    "                new_row['column_name'] = path[-1]\n",
    "                new_rows.append(new_row)\n",
    "                break\n",
    "\n",
    "            elif row[f\"col_path_{i}_LIST\"] == \"NO LIST\":\n",
    "                var_type = 'static'\n",
    "                subfield_path = path\n",
    "                column_name = path[-1]\n",
    "\n",
    "            elif row[f\"col_path_{i}_LIST\"] == 'nan':\n",
    "                var_type = 'skip'\n",
    "                subfield_path = path\n",
    "                column_name = path[-1]\n",
    "               \n",
    "            else:\n",
    "                var_type = 'skip'\n",
    "                subfield_path = path\n",
    "                column_name = path[-1]\n",
    "\n",
    "\n",
    "           \n",
    "        df.at[ix, 'subfield_path'] = subfield_path\n",
    "        df.at[ix, 'column_name'] = column_name\n",
    "        df.at[ix, 'var_type'] = var_type\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    new_rows.clear()\n",
    "    df = df.astype(str)\n",
    "    df_del = pd.DataFrame(delete_rows).astype(str)\n",
    "\n",
    "    df = pd.concat([df, df_del]).drop_duplicates(keep=False).reset_index(drop=True)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "      \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_and_store()\n",
    "This function orders the columns in the DataFrame, resets the index, fills the NA and saves the DataFrame as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_store(df, df_no_data, file_name):\n",
    "\n",
    "    \"\"\"\n",
    "    df: The dataframe that will be cleaned and stored\n",
    "    file_name: The filename of data structure that is being processed\n",
    "    \"\"\"\n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "    df = pd.concat([df, df_no_data], ignore_index=True)\n",
    "  \n",
    "    df = df[['json_name','column_name', 'path', 'list_path', 'subfield_path', 'var_type', 'data_type']]\n",
    "    df = df.astype(str)\n",
    "    df = df.drop_duplicates()\n",
    "    # Save the DataFrame \n",
    "    df.to_csv(f\"{main_path}Twitter/Output/Output_\" + file_name + '.csv', index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### structure_donations()\n",
    "The structure_donations() function executes all functions above and results in a saved DataFrame for each data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "def structure_donations(data, col_path, max_columns):\n",
    "     # Store the path to the data structure\n",
    "    data = Path(data)  \n",
    "    \n",
    "    # Save teh file name of the data structure\n",
    "    file_name = Path(data).stem \n",
    "\n",
    "    df, df_no_data = loading_data(data)\n",
    "    print('FINISH: loading_data()')\n",
    "    df = flatten_json(df)\n",
    "    print('FINISH: flatten_json()')\n",
    "    df = process_col_path(df, col_path_values, data_types)\n",
    "    print('FINISH: process_col_path()')\n",
    "    df = remove_intermediate_rows(df)\n",
    "    print('FINISH: remove_intermediate_rows()')\n",
    "    df = extract_path(df, max_columns)\n",
    "    print('FINISH: extract_path()')\n",
    "    df = list_path(df)\n",
    "    print('FINISH: list_path()')\n",
    "    df = clean_and_store(df, df_no_data, file_name)\n",
    "    print('FINISH: clean_and_store()')\n",
    "\n",
    "    del df,df_no_data, data\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute 'structure_donations()': Transform data structures from JSON format to tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_structure_twitter-2024-11-21-326363ec5310e8b3e585b6f4cde18bce0ff75734a36f376d0a2b5b4fc58916da (copy).json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n",
      "FINISH: flatten_json()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_317369/1142702742.py:97: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "X_structure_twitter-2025-03-15-76701c7591722b08ff3886a6ec1a130366b8ce153f72f649e364ff2f3ecf77bb.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n",
      "FINISH: flatten_json()\n",
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "X_structure_twitter-2024-11-21-326363ec5310e8b3e585b6f4cde18bce0ff75734a36f376d0a2b5b4fc58916da.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_317369/1142702742.py:97: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(r'^\\s*$', np.nan, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH: flatten_json()\n",
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n",
      "X_structure_twitter-2025-03-14-27da9bbce1b35bfe71f31166f8e683da63bb589a63202a59ef59c0909e6c2655.json\n",
      "<class 'dict'>\n",
      "FINISH: loading_data()\n",
      "FINISH: flatten_json()\n",
      "FINISH: process_col_path()\n",
      "FINISH: remove_intermediate_rows()\n",
      "FINISH: extract_path()\n",
      "FINISH: list_path()\n",
      "FINISH: clean_and_store()\n"
     ]
    }
   ],
   "source": [
    "# Execute the 'structure_donations()' function for each file (data structure) in the input directory\n",
    "for file in input_directory.iterdir():  \n",
    "    if file.is_file():  \n",
    "        print(file.name)\n",
    "        result = structure_donations(file, col_path, max_columns)\n",
    "        del result\n",
    "        gc.collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all data structures into one schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>json_name</th>\n",
       "      <th>column_name</th>\n",
       "      <th>path</th>\n",
       "      <th>list_path</th>\n",
       "      <th>subfield_path</th>\n",
       "      <th>var_type</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>manifest.js</td>\n",
       "      <td>accountId</td>\n",
       "      <td>['userInfo', 'accountId']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['userInfo', 'accountId']</td>\n",
       "      <td>skip</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>manifest.js</td>\n",
       "      <td>userName</td>\n",
       "      <td>['userInfo', 'userName']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['userInfo', 'userName']</td>\n",
       "      <td>skip</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>manifest.js</td>\n",
       "      <td>displayName</td>\n",
       "      <td>['userInfo', 'displayName']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['userInfo', 'displayName']</td>\n",
       "      <td>skip</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>manifest.js</td>\n",
       "      <td>sizeBytes</td>\n",
       "      <td>['archiveInfo', 'sizeBytes']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['archiveInfo', 'sizeBytes']</td>\n",
       "      <td>skip</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>manifest.js</td>\n",
       "      <td>generationDate</td>\n",
       "      <td>['archiveInfo', 'generationDate']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['archiveInfo', 'generationDate']</td>\n",
       "      <td>skip</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3578</th>\n",
       "      <td>saved-search.js</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['savedSearch', 'query']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3582</th>\n",
       "      <td>connected-application.js</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['connectedApplication', 'organization', 'priv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3583</th>\n",
       "      <td>connected-application.js</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['connectedApplication', 'organization', 'term...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>ip-audit.js</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3890</th>\n",
       "      <td>screen-name-change.js</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1382 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     json_name     column_name  \\\n",
       "0                  manifest.js       accountId   \n",
       "1                  manifest.js        userName   \n",
       "2                  manifest.js     displayName   \n",
       "3                  manifest.js       sizeBytes   \n",
       "4                  manifest.js  generationDate   \n",
       "...                        ...             ...   \n",
       "3578           saved-search.js             NaN   \n",
       "3582  connected-application.js             NaN   \n",
       "3583  connected-application.js             NaN   \n",
       "3872               ip-audit.js             NaN   \n",
       "3890     screen-name-change.js             NaN   \n",
       "\n",
       "                                                   path list_path  \\\n",
       "0                             ['userInfo', 'accountId']       NaN   \n",
       "1                              ['userInfo', 'userName']       NaN   \n",
       "2                           ['userInfo', 'displayName']       NaN   \n",
       "3                          ['archiveInfo', 'sizeBytes']       NaN   \n",
       "4                     ['archiveInfo', 'generationDate']       NaN   \n",
       "...                                                 ...       ...   \n",
       "3578                           ['savedSearch', 'query']       NaN   \n",
       "3582  ['connectedApplication', 'organization', 'priv...       NaN   \n",
       "3583  ['connectedApplication', 'organization', 'term...       NaN   \n",
       "3872                                                NaN       NaN   \n",
       "3890                                                NaN       NaN   \n",
       "\n",
       "                          subfield_path var_type data_type  \n",
       "0             ['userInfo', 'accountId']     skip    string  \n",
       "1              ['userInfo', 'userName']     skip    string  \n",
       "2           ['userInfo', 'displayName']     skip    string  \n",
       "3          ['archiveInfo', 'sizeBytes']     skip    string  \n",
       "4     ['archiveInfo', 'generationDate']     skip    string  \n",
       "...                                 ...      ...       ...  \n",
       "3578                                NaN      NaN    string  \n",
       "3582                                NaN      NaN    string  \n",
       "3583                                NaN      NaN    string  \n",
       "3872                                NaN      NaN       NaN  \n",
       "3890                                NaN      NaN       NaN  \n",
       "\n",
       "[1382 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to the folder containing CSV files\n",
    "output_path = f\"{main_path}Twitter/Output\"\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = list(Path(output_path).glob(\"*.csv\"))\n",
    "\n",
    "# Load all CSVs into a list of DataFrames\n",
    "dfs = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# Drop rows that are completely identical across all columns\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "display(merged_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Id creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df['name'] = merged_df['json_name'].str.replace(\".js\", \"\")\n",
    "\n",
    "id_df = merged_df[['name', 'path']]\n",
    "id_df = id_df.drop_duplicates()\n",
    "id_df['id'] = ''\n",
    "\n",
    "\n",
    "for n in range(1,max_columns):\n",
    "    duplicates = id_df[id_df.duplicated(subset='id', keep=False)]\n",
    "\n",
    "    for i, row in duplicates.iterrows():\n",
    "        path = row['path']\n",
    "        name = row['name']  \n",
    "\n",
    "        if pd.notna(path):\n",
    "            \n",
    "            path =  ast.literal_eval(path)\n",
    "            \n",
    "            path_rest = path[-n:]\n",
    "            \n",
    "\n",
    "            if not isinstance(path_rest, list):\n",
    "                path_rest = [path_rest]\n",
    "\n",
    "            id_list = [name] + path_rest\n",
    "\n",
    "            \n",
    "            new_id = ':'.join(id_list)\n",
    "\n",
    "        else:\n",
    "\n",
    "            new_id = name\n",
    "\n",
    "        id_df.at[i, 'id'] = new_id  \n",
    "\n",
    "\n",
    "merged_df = pd.merge(merged_df, id_df, on = ['name', 'path'], how = 'left')\n",
    "\n",
    "col = merged_df.pop('id') \n",
    "merged_df.insert(0, 'id', col) \n",
    "#merged_df['id']= merged_df['id'].fillna(merged_df['variable'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID duplicate flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_317369/29815283.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dup['duplicate_flag'] = 'Yes'\n"
     ]
    }
   ],
   "source": [
    "merged_df = merged_df.drop_duplicates()\n",
    "dup = merged_df[merged_df.duplicated('id', keep= False)]\n",
    "dup['duplicate_flag'] = 'Yes'\n",
    "dup = dup[['id', 'duplicate_flag']]\n",
    "merged_df = pd.merge(merged_df, dup, on = 'id', how = 'left')\n",
    "merged_df['duplicate_flag'] = merged_df['duplicate_flag'].fillna(value = 'No')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "keep_columns = ['json_name','id', 'column_name', \n",
    "                 'path', 'list_path', 'subfield_path', \n",
    "                 'var_type', 'data_type', 'json_name', 'duplicate_flag']\n",
    "\n",
    "\n",
    "merged_df = merged_df[keep_columns ]\n",
    "\n",
    "# Save the final merged DataFrame\n",
    "merged_df.to_csv(f\"{main_path}Twitter/Final/X_Merged_structures.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-donations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
